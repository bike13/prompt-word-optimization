# ÂèåËØ≠ÊñáÊ°£

## ÂéüÊñá

ReasoningBank : Scaling Agent
Self-Evolving with Reasoning Memory
Siru Ouyang1*, Jun Yan2, I-Hung Hsu2, Yanfei Chen2, Ke Jiang2, Zifeng Wang2, Rujun Han2, Long T. Le2,
Samira Daruki2, Xiangru Tang3, Vishy Tirumalashetty2, George Lee2, Mahsan Rofouei4, Hangfei Lin4, Jiawei
Han1, Chen-Yu Lee2and Tomas Pfister2
1University of Illinois Urbana-Champaign,2Google Cloud AI Research,3Yale University,4Google Cloud AI
With the growing adoption of large language model agents in persistent real-world roles, they naturally
encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the
accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We
propose ReasoningBank , a novel memory framework that distills generalizable reasoning strategies
from an agent‚Äôs self-judged successful and failed experiences. At test time, an agent retrieves relevant
memories from ReasoningBank to inform its interaction and then integrates new learnings back,
enabling it to become more capable over time. Building on this powerful experience learner, we further
introduce memory-aware test-time scaling ( MaTTS), which accelerates and diversifies this learning
process by scaling up the agent‚Äôs interaction experience. By allocating more compute to each task, the
agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing
higher-quality memory. The better memory in turn guides more effective scaling, establishing a pow-
erful synergy between memory and test-time scaling. Across web browsing and software engineering
benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store
raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS
further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling
dimension, enabling agents to self-evolve with emergent behaviors naturally arise.
1. Introduction
Therapidadvancementoflargelanguagemodels(LLMs)hassignificantlyacceleratedthedevelopment
of LLM agents (Liu et al., 2025a; Wang et al., 2024), which are crucial for tackling complex real-world
tasks that require multi-step interactions with environments, including web browsing (Gur et al.,
2024) and computer use (Xie et al., 2024; Yang et al., 2024). As these agents are increasingly
deployed in persistent, long-running roles, they naturally encounter a continuous stream of tasks
throughout their lifetime. However, they largely fail to learn from their accumulated experience
across tasks. By approaching each task in isolation, they are doomed to repeat past errors (Yin et al.,
2025), discard valuable insights from related problems, and lack self-evolving capabilities that make
the agent system more capable over time (Gao et al., 2025). This highlights the necessity of building
memory-aware agent systems that could learn from their past experiences (Zhang et al., 2024b).
Recenteffortsonagentmemoryhaveprimarilyfocusedonstoringpastinteractionsforreuse(Chen
et al., 2025; Sun et al., 2025; Tang et al., 2025b). While useful, these approaches are often limited to
leveraging raw trajectories (Kagaya et al., 2024; Kong et al., 2025; Zheng et al., 2024) or common,
successful routines (i.e., workflows, procedures) (Fang et al., 2025; Wang et al., 2025d). These
approaches suffer from two fundamental drawbacks. First, they lack the ability to distill higher-
level, transferable reasoning patterns. Second, by over-emphasizing successful experiences, they
leave the valuable lessons from an agent‚Äôs own failures largely underexplored (Zhang et al., 2024a).
Consequently, existing memory designs often remain limited to passive record-keeping rather than
Corresponding authors: siruo2@illinois.edu, {junyann, chenyulee}@google.com
* This work was done while Siru interned at Google Cloud AI Research.arXiv:2509.25140v1  [cs.AI]  29 Sep 2025
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
010203040Number of Tested TasksNumber of Successful Tasks036121518
Top-3 best-selling products in Jan 2023
Best-selling product type in Jan 2023
Top-2 searches in the store
Num of reviews mentioning ‚Äúdecent‚Äù
Monthly counts of top-3 products for 01/2023-05/2023 in MM:COUNT format Grand total of invoice
Trajectory Memory
ReasoningBank Memory
WorkÔ¨Çow Memory
<action> click('188') </action>  <think> I have successfully navigated to the 'My Account' page. The user wants to know the date of their first purchase. On this page, I see a "Recent Orders" section. To ensure I find the *first* purchase, I need to view the complete order history, not just the recent ones. There is a "View All" link next to "Recent Orders" which should take me to the full order history. </think> <action> click('1530') </action>  <think> I am on the "My Orders" page. The user wants to know the date of their first purchase. The orders are currently displayed from most recent to oldest. ‚Ä¶</think>  <action> click('1614') </action>
Earliest order for customer
WorkÔ¨Çow: Retrieve Information from Specific Page Section  <think> This workflow is designed to access a specific section ‚Ä¶ </think>  <action> click(‚Äòsection_or_tab_id') </action> <action> send_msg_to_user('extracted_info_message') </action>## Title Navigation Strategy ## Description When searching for specific information within history ‚Ä¶ ## Content ‚Ä¶ 1. Detect pagination mode and examine all items in relevant orders, ‚Ä¶; 2. Avoid infinite scrolls, use fallbacks if primary mode fails, ‚Ä¶; 3. Cross-reference with common ‚Ä¶Solution: ReasoningBankWe get rich task insights from expensive test-time exploration / scaling.  We store them in a ReasoningBank.  When a new task comes, we incorporate the relevant insights to aid decision making.
9
ReasoningBankNo Memory
Figure1|ReasoningBank inducesreusablereasoningstrategies, makingmemoryitemsmoretrans-
ferrable for future use. This enables agents to continuously evolve and achieve higher accumulative
success rates than the ‚ÄúNo Memory‚Äù baseline on the WebArena-Admin subset.
providing actionable, generalizable guidance for future decisions.
To bridgethis gap, wepropose ReasoningBank , anovel memory framework for agent systems.
ReasoningBank distills and organizes memory items from both successful and failed experiences
judged by the agent itself without ground-truth labels. As shown in Figure 1, it captures not only
effective strategies from successes but also crucial preventative lessons from failures, abstracting them
into a collection of actionable principles. This process operates in a closed loop: when facing a new
task, the agent retrieves relevant memories from ReasoningBank to guide its actions. Afterward,
thenewexperienceisanalyzed, distilled, andconsolidatedbackintothe ReasoningBank , allowing
the agent to continuously evolve and improve its strategic capabilities.
With ReasoningBank as a strong experience learner, we study experience scaling to establish
a powerful synergy between memory and test-time scaling . Instead of scaling experience through
breadth by adding more tasks, we focus on scaling experience through depth by tackling each single
task with more exploration. We introduce memory-aware test-time scaling ( MaTTS ) in both parallel
and sequential settings, which generates diverse exploration to provide contrastive signals, enabling
ReasoningBank to synthesize more generalizable memories. It creates a synergy between memory
and test-time scaling: high-quality memory steers the scaled exploration toward more promising
paths, while the rich experiences generated forge even stronger memories. This positive feedback
loop positions memory-driven experience scaling as a new scaling dimension for agents.
We conduct extensive experiments on challenging benchmarks for web browsing (WebArena,
Mind2Web) and software engineering (SWE-Bench-Verified). We demonstrate that our approaches
outperform baselines in both effectiveness (up to 34.2% relative improvement, Figure 4(b)) and
efficiency (16.0% less interaction steps, Table 1). Specifically, ReasoningBank synergizes best
with MaTTS , making it an essential component for memory-driven experience scaling.
Our contributions are threefold: (1) We propose ReasoningBank , a novel memory framework
that distills generalizable reasoning strategies from both successful and failed experiences, beyond
prior work limited to raw trajectories or success-only routines. (2) We introduce MaTTS that creates
a powerful synergy between memory and test-time scaling, establishing memory-driven experience
as a new scaling dimension for agents. (3) We demonstrate through extensive experiments that our
approaches not only improve effectiveness and efficiency over existing methods, but also enable agents
to learn from failures and develop increasingly complex, emergent reasoning strategies over time.
2
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
2. Related Work
Memory for LLM Agents. Memory has emerged as an essential module in modern agent sys-
tems (Zhang et al., 2024b) to enhance their performance by utilizing past information (Zhang
et al., 2024b). Existing memory systems organize and store information in various forms, including
plain text (Packer et al., 2023), latent knowledge embeddings (Wang et al., 2025b) and structured
graphs (Chhikara et al., 2025; Li et al., 2025b; Xu et al., 2025). Beyond memory content, those
methods usually involve retrieval mechanisms (e.g., semantic search) with memory management
strategies (e.g., updating) (Hu et al., 2025a; Tan et al., 2025). More recently, with the growing
development of reinforcement learning (RL) in LLM agents, RL has also been leveraged for memory
management in agent systems (Yu et al., 2025a; Zhou et al., 2025). While most efforts primarily
emphasizing personalization (Zhang et al., 2025; Zhong et al., 2024) and long-context manage-
ment (Hu et al., 2025b; Maharana et al., 2024; Wu et al., 2025), this paper falls in the research line
of learning from past experiences (SU et al., 2025; Zhao et al., 2024) as memory, which is a critical
aspect for developing self-evolving agent systems (Gao et al., 2025; Liang et al., 2024). Different from
previous works that emphasize reusing successful trajectories (Tang et al., 2025a; Zheng et al., 2024)
or procedural workflows (Fang et al., 2025; Liu et al., 2025b; Qian et al., 2024; Wang et al., 2025d),
ReasoningBank stores high-level strategies and reasoning hints. By abstracting experiences into
reusable reasoning units, ReasoningBank enables agents to generalize not only from successful
cases but also by learning from failures, thereby providing richer guidance for test-time learning.
Additionally, we are the first to explore memory-aware test-time scaling, where ReasoningBank
synergistically work with diverse signals from abundant exploration trajectories.
Agent Test-Time Scaling. Test-time scaling (TTS) (Snell et al., 2025) has demonstrated strong
effectiveness and has become a widely adopted practice in end-to-end problem-solving such as
coding (Li et al., 2025a; Yu et al., 2025c) and math reasoning (Muennighoff et al., 2025), where
methods including best-of-N (Chow et al., 2025), beam search (Wu et al., 2024b), and leveraging
verifiers(Setluretal.,2025)arecommonlyemployed. However,itsapplicationtomulti-turninteractive
scenarios, particularly agentic tasks, remains underexplored. Existing works mainly adapt the lesson
learned from reasoning tasks (Zhu et al., 2025b) and scale different dimensions of agentic systems,
including the search space for each action (Yu et al., 2025b), the number of agents in multi-agent
systems (Jin et al., 2025), and the number of interactions with the environment (Shen et al., 2025).
We found that none of these efforts considers the role of agent memory in scaling, where an agent
can learn from past experiences to guide future decisions. Our work extends this line of research
by introducing memory-aware test-time scaling ( MaTTS ). As we will show in our empirical results
(¬ß4.3 and ¬ß4.4), memory offers benefits beyond mere computational scaling, where memory and
scaling synergistically work towards better performance.
3. Methodology
In this section, we introduce the problem setup (¬ß3.1), and present our proposed ReasoningBank
(¬ß3.2), based on which we further develop memory-aware test-time scaling ( MaTTS ) (¬ß3.3).
3.1. Problem Formulation
Agent Configuration. The scope of this work focuses on LLM-based agents. The agent policy
ùúãL(¬∑|M ,A)is parameterized by the backbone LLM L, conditioned on a memory module M, and the
action spaceA, denoted as ùúãLfor short. The agent needs to perform a task via interacting with the
environment, which can be viewed as a sequential decision-making process. Formally, the transition
function of the environment is defined as T(ùë†ùë°+1|ùë†ùë°, ùëéùë°)where ùë†ùë°is the state and ùëéùë°is the action selected
3
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
TimeTask q1Task q2‚Ä¶‚Ä¶‚Ä¶‚Ä¶Task qNTask qi
MethodologyProblem Formulation: Streaming nature of testing tasks  Given a test set of tasks , the agent works on the tasks sequentially:  ‚Ä¢  ‚Ä¢No ground-truth feedback during testing{t1,t2,‚ãØ,tn}t1‚Üít2‚Üí‚ãØ‚Üítn
qiAgent
Env
ReasoningBank:  a collection of memory items
: experience/trajectory‚Ñã(i) Memory Retrieval
Memory Items(ii) Memory Extraction(iii) Memory ConsolidationTask : Tell me the status of my latest order and when it will arriveqi
Experience/Trajectory ‚Ñã<think>‚Ä¶ I need to navigate to ‚Ä¶</think> <action> click(‚Äò188‚Äô) </action>
Memory Item j Title: Prioritize user account sections for personal dataDescription When a query requests user-speciÔ¨Åc‚Ä¶Content: Systematically look for and click on links ‚Ä¶Memory Extractor
Figure 2|Overview of ReasoningBank . Experiences are distilled into structured memory items
with a title, description, and content. For each new task, the agent retrieves relevant items to interact
with the environment, and constructs new ones from both successful and failed trajectories. These
items are then consolidated into ReasoningBank , forming a closed-loop memory process.
byùúãLat time ùë°. We focus on web browsing and software engineering (SWE) tasks. Ais a set of web
navigation operations for web browsing and bash commands for SWE tasks, MisReasoningBank
and initialized as empty. For each given task, the agent generates a trajectory of (ùëú0:ùë°, ùëé0:ùë°)forùë°steps,
where observation ùëúùë°is from the current state ùë†ùë°. Observations are text-based accessibility tree of web
pages1for web browsing tasks and code snippets for SWE. The agent needs to generate an action
ùëéùë°+1‚ààAviaùúãL(ùëú0:ùë°, ùëé0:ùë°;M,A)‚Üí ùëéùë°+1. For implementation, the memory module Mcontributes
relevant memories as additional system instruction for ùúãL.
Test-Time Learning. We focus on the test-time learning paradigm (Wang et al., 2025c; Wu et al.,
2024a) where a sequence of task queries Q={ùëû1, ùëû2, ..., ùëû ùëÅ}arrives in a streaming fashion, i.e., each
query is revealed and must be completed sequentially without access to future ones. In this setting,
no ground truth is available during test-time, so the agent must continually evolveby only leveraging
its own past trajectories and any self-verification without relying on external labels. This streaming
setting highlights two key challenges: (i) how to extract and preserve useful memory from past
trajectories, and (ii) how to effectively leverage such memory for future queries to avoid redundantly
re-discovering already successful strategies or repeating past mistakes.
3.2.ReasoningBank
Past raw trajectories (or experiences), while being comprehensive and original, are often too lengthy
andnoisytobedirectlyappliedtothecurrentuserquery. AsillustratedinFigure2, ReasoningBank
distills useful strategies and reasoning hints from past experiences into structured memory items,
which are then stored for future reuse.
Memory Schema. Memory items in ReasoningBank are designed and induced from past experi-
ences as structured knowledge units that abstract away low-level execution details while preserving
transferrable reasoning patterns and strategies. Each memory item specifies three components: (i)
atitle, which serves as a concise identifier summarizing the core strategy or reasoning pattern;
(ii) adescription , which provides a brief one-sentence summary of the memory item; and (iii) the
content, which records the distilled reasoning steps, decision rationales, or operational insights ex-
tracted from past experiences. Together, memory items extracted are both human-interpretable and
1Weusethethinkingprocessof ùúãLastheapproximationof ùëú0:ùë°duetolengthyobservationrepresentationsfollowingWang
et al. (2025d).
4
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
machine-usable, facilitating efficient usage and integration with agents.
Integration of ReasoningBank with Agents. An agent equipped with ReasoningBank can
draw upon a curated pool of transferable strategies to guide decision-making. This enables the agent
to recall effective insights, avoid previously observed pitfalls, and adapt more robustly to unseen
queries. The integration proceeds in three steps: (i) memory retrieval , (ii)memory construction ,
and (iii) memory consolidation , as shown in Figure 2. During memory retrieval , the agent queries
ReasoningBank with the current query context to identify the top- ùëòrelevant experiences and their
corresponding memory items using embedding-based similarity search. Retrieved items are injected
into the agent‚Äôs system instruction, ensuring that the decision-making is grounded with useful past
experiences. When the current query task is completed, we will perform memory construction to
extract new memory items. The first step is to obtain proxy signals for the correctness of completed
trajectories: we adopt an LLM-as-a-judge (Gu et al., 2024) to label outcomes as success or failure
given the query and trajectory, without access to any ground-truth. Based on these signals, we apply
different extraction strategies: successful experiences contribute validated strategies, while failed
ones supply counterfactual signals and pitfalls that help sharpen guardrails. In practice, we extract
multiple memory items for each trajectory/experience as detailed in Appendix A.1. Finally, memory
consolidation incorporates these items into ReasoningBank with a simple addition operation,
maintaining an evolving repository of memory items. Details are in Appendix A.2. Together, these
steps form a closed-loop process: the agent leverages past experiences, constructs new memory
from current tasks, and continually updates its memory, enabling sustained evolvement in test-time
learning scenarios.2
3.3.MaTTS: Memory-aware Test-Time Scaling
ReasoningBank enables learning from experiences to translate more experiences into greater
improvements. As test-time scaling (Snell et al., 2025) recently emerged as a powerful strategy for
boosting the performance of LLM agents (Zhu et al., 2025a), it shows strong potential by allocating
additional inference-time computation to generate abundant exploration histories. A direct combina-
tion of ReasoningBank and test-time scaling is depicted in Figure 3(a), where more trajectories
are independently converted to more memory items. However, this vanilla form is suboptimal because
it does not leverage inherent contrastive signal that arises from redundant exploration on the same
problem, which limits the resulting performance advantage brought by test-time scaling. To address
this, we propose Memory-aware Test-Time Scaling (MaTTS ), a novel integration of test-time scaling
with ReasoningBank . Unlike the vanilla approach, MaTTS deliberately learns from the abundant
successful and failure trajectories generated during scaling for more effective memory curation. We
design two complementary instantiations for MaTTS , parallel scaling and sequential scaling, as
illustrated in Figure 3(b) and 3(c) with detailed implementation in Appendix A.3.
Parallel Scaling. In the parallel setting, we generate multiple trajectories for the same query under
the guidance of retrieved memory items. By comparing and contrasting ( self-contrast (Chen et al.,
2020))across different trajectories , the agent can identify consistent reasoning patterns while filtering
out spurious solutions. This process enables more reliable memory curation from multiple trials of a
single query that promotes diverse exploration.
Sequential Scaling. We iteratively refines its reasoning within a single trajectory after the initial
completion, following the principle of self-refinement (Madaan et al., 2023). During this process, the
intermediate notes generated in self-refinement are also used as valuable signals for memory, since
2We deliberately keep the memory usage pipeline simple, avoiding additional complexity in retrieval or consolidation so
as to highlight the contribution of ReasoningBank itself. These components, however, can be further enhanced with
more sophisticated techniques, which could provide additional benefits.
5
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
MethodologyMemory-aware test-time scaling (MaTS): Leverage contrastive signals from past experiences: ‚Ä¢Parallel Scaling: use self-contrast for memory curation ‚Ä¢Sequential Scaling: use self-refinement for memory curation
Task qiTask qi+1
Current memory
New memory
Self-ReÔ¨ÅneTrajTraj‚Äô‚Ä¶
(c) MaTTS - SequentialTask qiTask qi+1(b) MaTTS - Parallel
Current memoryTrajectory 1Trajectory 2Trajectory n
New memory
Self-Contrast‚Ä¶‚Ä¶
Traj 1Traj 2Traj nNew Mem 1
Mem 1New Mem 2
New Mem n
Mem 2
Mem n‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶Task qiTask qi+1(a) Vanilla TTS (MaTTS w/o aggregation)
Figure 3|Comparison of (a) vanilla TTS andMaTTS with(b) parallel scaling , where self-contrast
across multiple trajectories curates reliable memory, and (c) sequential scaling , where self-refinement
enriches memory with intermediate reasoning signals.
Table 1|Experiment results of ReasoningBank on WebArena benchmark. Success rate (SR ‚Üë)
and the number of steps (Step ‚Üì) are reported on 5 subsets for 3 different backbone LLMs.
ModelsShopping Admin Gitlab Reddit Multi Overall
(187) (182) (180) (106) (29) (684)
SR Step SR Step SR Step SR Step SR Step SR Step
Gemini-2.5-flash
No Memory 39.0 8.2 44.5 9.5 33.9 13.3 55.7 6.7 10.3 10.0 40.5 9.7
Synapse 40.6 7.0 45.1 9.1 35.6 13.0 59.4 6.5 10.3 10.5 42.1 9.2
AWM 44.4 7.0 46.7 8.8 37.2 13.2 62.3 6.1 3.4 7.744.1 9.0
ReasoningBank 49.7 6.1 51.1 8.2 40.6 12.3 67.0 5.6 13.8 8.848.8 8.3
Gemini-2.5-pro
No Memory 45.5 7.6 51.1 8.7 35.0 11.6 71.7 6.0 6.9 8.8 46.7 8.8
Synapse 46.5 6.6 52.2 8.9 38.3 11.3 68.9 5.9 6.9 9.0 47.7 8.5
AWM 48.1 6.4 49.3 9.8 40.0 11.2 68.9 6.4 3.4 9.3 47.6 8.7
ReasoningBank 51.9 6.0 56.6 7.7 44.4 9.8 80.2 5.1 13.8 8.2 53.9 7.4
Claude-3.7-sonnet
No Memory 38.5 6.1 49.5 8.4 36.7 10.6 53.8 5.5 0.0 11.6 41.7 8.0
Synapse 39.6 5.8 50.5 8.5 38.0 10.0 53.8 6.1 0.0 11.8 42.6 7.9
AWM 39.6 7.2 47.8 9.3 34.6 10.9 52.8 7.0 0.0 12.4 40.8 8.9
ReasoningBank 44.9 5.6 53.3 7.6 41.1 9.5 57.5 5.2 3.4 10.5 46.3 7.3
they capture reasoning attempts, corrections, and insights that may not appear in the final solution.
We define the scaling factor ùëò, denoting the number of trajectories for parallel scaling and refine-
ment steps for sequential scaling. Equipped with ReasoningBank , both parallel and sequential
strategies become memory-aware, ensuring that the additional computation allocated at test time
translates into more transferable and higher-quality memory for future tasks.
4. Experiments
4.1. Setup
Following existing work (Wang et al., 2025d), we conduct experiments on WebArena (Zhou et al.,
6
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
2024) which features general web navigation across diverse domains,3and Mind2Web (Deng et al.,
2023) that tests generalization of agents on versatile operations and environments. We also conduct
experiment on SWE-Bench-Verified (Jimenez et al., 2024) for repository-level issue-resolving. For
comparison, we consider baselines ranging from memory-free agents (No Memory) to trajectory-based
memory (Synapse) (Zheng et al., 2024) and workflow-based memory (AWM) (Wang et al., 2025d).
Our agents are built on Gemini-2.5 (Comanici et al., 2025) and Claude-3.7 (Anthropic, 2025) models
in BrowserGym (de Chezelles et al., 2025) environment for web browsing and bash-only environment
for SWE, following ReAct (Yao et al., 2023) style with default decoding configurations. Evaluation
focuses on effectiveness (success rate) and efficiency (average interaction steps), with specific metrics
varying for each dataset. Full descriptions for datasets, baselines, implementations, and evaluation
protocols are in Appendix B.
4.2. Results of ReasoningBank
Tables 1, 2, 3 summarize the main evaluation results of ReasoningBank on WebArena, Mind2Web,
and SWE-Bench-Verified. We have the following observations.
ReasoningBank consistently outperforms baselines across LLM backbones on all datasets.
Specifically, ReasoningBank improves the overall success rate on WebArena (Table 1) by +8.3,
+7.2, and+4.6with three different backbone LLMs compared to memory-free agents. A similar
pattern holds on Mind2Web (Table 3), where ReasoningBank delivers clear gains across cross-
task, cross-website, and cross-domain settings, underscoring both the consistency and scalability
of its benefits across datasets and model sizes. Results on SWE-Bench-Verified (Table 2) further
confirm its robustness. Crucially, unlike baselines such as Synapse and AWM that rely on a narrow,
homogeneous memory source derived exclusively from successful trajectories, ReasoningBank
employs a superior extraction strategy that is key to its consistent outperformance.
Table 2|Experiment results of Rea-
soningBank on SWE-Bench-Verified
dataset for issue-resolving in a given
repository.
Methods Resolve Rate Step
Gemini-2.5-flash
No Memory 34.2 30.3
Synapse 35.4 30.7
ReasoningBank 38.8 27.5
Gemini-2.5-pro
No Memory 54.0 21.1
Synapse 53.4 21.0
ReasoningBank 57.4 19.8ReasoningBank enhances generalization with bet-
ter transferrable memory across tasks. We also evalu-
ate in challenging generalization settings. On WebArena
(Table 1), the Multisubset requires transferring mem-
ory across multiple websites, where ReasoningBank
achieves a notable gain of +4.6averaged SR over the
strongest baseline. In contrast, strong baselines such as
AWM fail to provide gains and even degrade in this set-
ting. On Mind2Web (Table 3), which includes cross-task,
cross-website, and cross-domain evaluations that impose
progressively higher demands, ReasoningBank con-
sistently improves success rates. The gains are especially
pronounced in the cross-domain setting, which requires
the highest level of generalization. These results demon-
stratethatmemorycuratedby ReasoningBank ismore
robust and transferable, enabling agents to generalize effectively across diverse scenarios.
ReasoningBank achieves superior efficiency by leveraging past experiences as memory. In
addition to higher success rates, ReasoningBank also reduces the number of interaction steps
needed to complete tasks, as shown in the Stepmetric of Table 1 and 2. On WebArena, across
almost all subsets and backbones, ReasoningBank lowers the average step count by up to 1.4
compared with ‚ÄúNo Memory‚Äù, and 1.6compared with other memory baselines. The average step
3We exclude the domain of Mapdue to website issues following Miyai et al. (2025) for a fair comparison.
7
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
Table 3|Results on Mind2Web benchmark for cross-task, cross-website, and cross-domain generaliza-
tion test. EA (‚Üë) is short for element accuracy, AF 1(‚Üë) is short for action F 1, and SSR (‚Üë) is short for
step success rate. SR ( ‚Üë) is the task-level success rate measuring if all steps are correct.
ModelsCross-Task Cross-Website Cross-Domain
(252) (177) (912)
EA AF 1SSR SR EA AF 1SSR SR EA AF 1SSR SR
Gemini-2.5-flash
No Memory 46.0 59.1 40.3 3.3 39.8 45.1 31.7 1.7 35.8 37.9 31.9 1.0
Synapse 47.0 59.5 41.2 3.5 40.3 46.0 32.1 1.9 36.3 38.5 32.4 1.1
AWM 46.3 56.1 41.0 3.5 39.1 42.2 31.7 2.1 33.3 36.5 30.1 0.7
ReasoningBank 52.1 60.4 44.9 4.8 44.3 52.6 33.9 2.3 40.6 41.3 36.6 1.6
Gemini-2.5-pro
No Memory 49.3 60.2 44.4 3.5 41.2 49.8 34.8 3.4 37.9 37.7 35.0 1.4
Synapse 50.1 61.0 44.7 3.6 41.8 51.2 35.0 3.2 38.5 39.8 35.6 1.5
AWM 48.6 61.2 44.4 3.7 41.9 47.9 34.8 2.3 37.3 38.1 34.4 1.2
ReasoningBank 53.6 62.7 45.6 5.1 46.1 54.8 36.9 3.8 42.8 45.2 38.1 1.7
on SWE-Bench-Verified is also smaller by saving 2.8and 1.3steps respectively. This indicates that
ReasoningBank enables agents to solve tasks more efficiently by reusing and refining reasoning
knowledge, thus avoiding unnecessary or redundant exploration.
4.3. Results of MaTTS
We experimented MaTTS with Gemini-2.5-flash on Webarena-Shopping subset. By default, MaTTS
integrates ReasoningBank , but it could also use other memory mechanisms. To investigate the
overall scaling effect, we benchmark with (i)MaTTS w/o memory , which represents the scaling
setting without memory mechanism, (ii)MaTTS w/o aggregation , which is equal to Vanilla TTS in
Figure 3(a) and (iii)MaTTS to demonstrate the effect with respect to scaling factor ùëò. Notably, ùëò=1
is the setting without scaling. For parallel scaling, we compute Best-of-N (BoN) as the final metric
detailed in Appendix A.3. Results are shown in Figure 4.
Experiment ResultsEvaluation of MaTS
(b) Sequential Scaling364044485256
(a) Parallel ScalingSuccess Rate (SR)40.641.742.239.439.049.750.349.752.454.055.151.352.952.4364044485256
40.640.138.537.439.049.751.953.554.054.551.952.451.950.812345MaTTSMaTTS w/o aggregationMaTTS w/o memory
12345
Figure 4|Effect of scaling factor ùëòforMaTTS under with Rea-
soningBank on WebArena-Shopping subset. We compare (a)
parallel and (b) sequential test-time scaling.Both parallel scaling and se-
quential scaling boost perfor-
mance. Increasing ùëògener-
ally improves success rate, con-
firming the benefit of allocating
more inference-time computa-
tion. With MaTTS , parallel scal-
ing grows from 49.7(ùëò=1) to
55.1(ùëò=5), while sequential
scaling rises from 49.7to54.5.
For the baseline of MaTTS w/o
memory, the gains are smaller
and less consistent (e.g., parallel
scaling fluctuates between 39.0
and 42.2, sequential between 37.4and 40.6). In contrast, MaTTS enables stronger and more stable
improvements across both scaling strategies, highlighting its role in making scaling more effective.
MaTTS is consistently better than vanilla TTS. With ReasoningBank ,MaTTS consistently
8
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
surpasses MaTTS w/o aggregation (vanilla TTS), showing that memory-aware coordination and
aggregationisimportant. Specifically,at ùëò=5,MaTTS achieves 55.1inparallelscalingcomparedwith
52.4for vanilla TTS, and 54.5versus 51.9in sequential scaling. These improvements highlight that
memory-aware scaling effectively directs the agent toward more promising solutions by synthesizing
insights from multiple trajectories or interaction steps to leverage contrastive signals.
Sequential scaling shows short-term advantage, but parallel dominates at larger scales for
ReasoningBank .With stronger memory mechanisms such as ReasoningBank , sequential
refinement brings higher gains at small ùëò, but its benefit quickly saturates‚Äîonce the model either
succeeds or fails decisively, further refinements add little new insight. In contrast, parallel scaling
continues to provide diverse rollouts that allow the model to critique and improve upon its own
generations, leading it to surpass sequential at larger ùëò(e.g., 55.1vs.54.5atùëò=5). In contrast, for
vanilla TTS which is not equipped with memory module, sequential scaling yields little or even no
benefit as scaling goes on, and parallel scaling consistently dominates.
4.4. Synergy of Memory and Test-Time Scaling
While the previous section establishes the overall effectiveness of MaTTS , we highlight the synergy
between memory and TTS in this section. Figure 5 presents a snapshot of MaTTS on the WebArena-
Shopping subset with parallel scaling factor ùëò=3, where we report both Pass@1 (randomly selected
trajectory)andBest-of-3(BoN).Thissettingallowsustoexaminethebidirectionalinteractionbetween
memory quality and scaling effectiveness.
Better memory enables stronger test-time scaling performance. To see how memory improves the
effectiveness of scaling, we focus on the BoN results, which directly measures an agent‚Äôs ability to
surface the best outcome among multiple rollouts. As shown by bluebars in Figure 5, the benefit
of scaling depends critically on the underlying memory. Without memory, scaling yields slight
improvement, with BoN rises only from 39.0to40.6. Weaker memory mechanisms such as Synapse
and AWM provide moderate gains, reaching 42.8and 45.5, respectively. In contrast, MaTTS with
ReasoningBank delivers the strongest benefit, with BoN climbing from 49.7to52.4. These results
show that high-quality memory directs scaling toward more promising rollouts, ensuring that the
additional trajectories are not wasted but converted into higher success rates.
3540455055
3540455055
No MemorySynapseAWMReasoningBank
No ScalingPass@1Best-of-3
39.040.644.449.740.640.141.250.842.845.552.4Success Rate (SR)Experiment ResultsSynergy of Memory and TTS
38.5
Figure 5|Snapshot of MaTTS on WebArena-
Shopping subset with different memory mecha-
nisms with ùëò=3. We compute BoN for all 3tra-
jectories and Pass@1 with one randomly selected
trajectory.Scaling yields better memory curation. To
fairlyevaluatehowscalingfeedsbackintomem-
ory, we report Pass@1, which measures the av-
erage quality of trajectories after memory cu-
ration and allows direct comparison with the
no-scaling case. The trend is depicted in pink
barsandisstriking: scalingactuallyreducesper-
formance for weaker memories, where Synapse
drops from 40.6to40.1, and AWM from 44.4
to41.2. These declines suggest that without
strong guidance, the extra rollouts generated
by scaling introduce noise rather than useful
signals. In contrast, ReasoningBank is the
only method that benefits: Pass@1 rises from
49.7to50.8, showing that high-quality memory
can harness the diversity of scaling to extract
constructive contrastive signals. This asymmetry highlights that scaling alone is insufficient; only
when paired with good memory mechanism does it contribute to curation of more effective memory,
9
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
thereby closing the virtuous cycle.
5. Analysis
We analyze ReasoningBank beyond overall benchmark performance through three aspects:
incorporating failure trajectories, examining emergent strategies, and evaluating efficiency across
both successful and failed cases. Additional analyses are in Appendix C.
Find reviewer who complain of customer serviceEmergent advanced strategiesRegularly cross-referencing the current view with the task requirements helps prevent errors and guides efficient navigation. If the current data doesn't align with expectations (e.g., contents are incorrect or irrelevant), reassess available navigation options such as specific page numbers, search filters, or alternative sections.
‚Ä¶, actively look for and click on ‚ÄùNext Page," "Page X," or "Load More" links.‚Ä¶ it‚Äôs crucial to Ô¨Årst re-check the element's current identifier ‚Ä¶
Procedural/execution strategyAtomic self-reflection
Test-time Learning TimelineBefore scanning, always leverage any available search or filter functionalities, ensure completeness before reporting ‚Ä¶Regularly cross-referencing the current view with the task requirements helps prevent errors‚Ä¶ If the current data doesn't align with expectations (e.g., contents are incorrect or irrelevant), reassess available options such as search filters,  alternative sections ‚Ä¶
Generalized complex/effective strategy
Evolved adaptive check
Figure 6|A case study illustrating emergent behaviors in ReasoningBank through memory items.
5.1. Emergent behaviors with ReasoningBank
We find that the strategies in ReasoningBank are not flat or monolithic, but instead evolve
over time, exhibiting emergent behaviors that resemble the learning dynamics of RL (Wang et al.,
2025a). As illustrated in Figure 6, a memory item in ReasoningBank could gradually evolve
during test-time learning process. It starts from execution-oriented or procedural strategies (e.g.,
find navigation links), where the agent follows straightforward action rules. It then progresses
to adaptive self-reflections such as re-verifying identifiers to reduce simple mistakes. With more
experiences, the same memory item evolves into adaptive checks, where the agent systematically
leverages available search or filters to ensure completeness before results. Finally, it eventually
matures into compositional strategies such as cross-referencing task requirements and reassessing
options. This evolution highlights how ReasoningBank enables agents to refine strategies from
low-level actions to high-level reasoning during test-time learning.
5.2. Incorporating failure trajectories
AnalysisAblation Study
353841444750
SynapseAWMReasoningBank
No mem: 39.0Success onlyw/ Failure40.644.446.541.742.249.7
3841444750
01234
Number of experiences3949.74645.544.4Success Rate
Success Rate
Figure 7|Ablation results of in-
corporating failure trajectories for
memory induction.Figure 7 compares different memory designs on WebArena-
Shopping with Gemini-2.5-flash under two settings: using only
successful trajectories versus leveraging both successes and
failures. Baseline methods such as Synapse and AWM build
memory solely from successful trajectories, and thus are not
equipped to benefit from failures. As a result, when failures are
added, their performance is limited or even degraded: Synapse
increases only from 40.6(success only) to 41.7(with failures),
while AWM drops from 44.4to42.2. In contrast, the design of
ReasoningBank enables distillation of reasoning patterns
frombothsuccessesandfailures,achieving 46.5onsuccess-only
tracesandfurtherimprovingto 49.7whenfailuresareincluded.
This highlights that, unlike baselines, ReasoningBank can
transform failures into constructive signals rather than noise, enabling more robust generalization.
10
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
Table 4|Average number of steps on successful and failed test instances across four WebArena
domains. ReasoningBank consistently reduces the number of steps compared to the vanilla
baseline, with notably larger reductions on successful instances.
ModelsShopping Admin Gitlab Reddit
Successful Failed Successful Failed Successful Failed Successful Failed
No Memory 6.8 8.7 8.4 10.4 8.6 15.7 6.1 7.6
ReasoningBank 4.7‚Üì2.17.3‚Üì1.47.0‚Üì1.49.5‚Üì0.97.6‚Üì1.015.5‚Üì0.25.0‚Üì1.16.8‚Üì0.8
5.3. Efficiency Study
While the overall number of steps in Table 1 provides a general view of model efficiency, it does
not distinguish whether reductions come from successful or failed trajectories. To gain deeper insight,
we further separate the analysis into successful and failed test cases, which allows us to understand
the source of step reduction: a desirable system should reduce unnecessary exploration when it is on
the right track, rather than merely cutting short failed attempts. The results are shown in Table 4. We
find that ReasoningBank consistently reduces the number of steps across all domains compared
to the baseline. More importantly, the reduction is particularly pronounced on successful cases,
reaching up to 2.1fewer steps (a 26.9%relative reduction) than on failed ones. This indicates that
ReasoningBank primarily helps the agent reach solutions with fewer interactions by strengthening
its ability to follow effective reasoning paths rather than simply truncating failed trajectories, which
highlight the role of memory in guiding purposeful decision-making and improving efficiency in
practice.
6. Conclusion
We introduce ReasoningBank , a memory framework that distills strategy-level reasoning signals
frombothsuccessesandfailuresandintegratesthemintotest-timescaling( MaTTS ).Extensiveexper-
iments show that ReasoningBank consistently improves performance while reducing redundant
exploration. Further results reveal a strong synergy between memory and scaling: ReasoningBank
guides scaling toward more promising rollouts, while diverse rollouts enrich memory with valuable
contrastive signals. We also provide analyses of individual components and emergent behaviors. Our
findings suggest a practical pathway toward building adaptive and lifelong-learning agents, with
additional future directions and limitations in Appendix D and E.
7. Acknowledgments
We thank Jiao Sun, Jing Nathan Yan, and members from Google Cloud AI Research for their valuable
feedback during the preparation of the paper.
References
Anthropic. Claude 3.7 sonnet and claude code, 2025. URL https://www.anthropic.com/news/
claude-3-7-sonnet .
S. Chen, S. Lin, X. Gu, Y. Shi, H. Lian, L. Yun, D. Chen, W. Sun, L. Cao, and Q. Wang. Swe-
11
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
exp: Experience-driven software issue resolution. ArXiv preprint , abs/2507.23361, 2025. URL
https://arxiv.org/abs/2507.23361 .
T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton. A simple framework for contrastive learning of
visual representations. In Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of Machine Learning Research ,
pages 1597‚Äì1607. PMLR, 2020. URL http://proceedings.mlr.press/v119/chen20j.html .
P. Chhikara, D. Khant, S. Aryan, T. Singh, and D. Yadav. Mem0: Building production-ready ai
agents with scalable long-term memory. ArXiv preprint , abs/2504.19413, 2025. URL https:
//arxiv.org/abs/2504.19413 .
Y. Chow, G. Tennenholtz, I. Gur, V. Zhuang, B. Dai, A. Kumar, R. Agarwal, S. Thiagarajan, C. Boutilier,
and A. Faust. Inference-aware fine-tuning for best-of-n sampling in large language models. In The
Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.
net/forum?id=77gQUdQhE7 .
G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram,
D. Zhang, E. Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality,
long context, and next generation agentic capabilities. ArXiv preprint , abs/2507.06261, 2025. URL
https://arxiv.org/abs/2507.06261 .
T. L. S. de Chezelles, M. Gasse, A. Lacoste, M. Caccia, A. Drouin, L. Boisvert, M. Thakkar, T. Marty,
R. Assouel, S. O. Shayegan, L. K. Jang, X. H. L√π, O. Yoran, D. Kong, F. F. Xu, S. Reddy, G. Neubig,
Q. Cappart, R. Salakhutdinov, and N. Chapados. The browsergym ecosystem for web agent research.
Transactions on Machine Learning Research , 2025. ISSN 2835-8856. URL https://openreview.
net/forum?id=5298fKGmv3 . Expert Certification.
X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2web: Towards
a generalist agent for the web. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt,
and S. Levine, editors, Advances in Neural Information Processing Systems 36: Annual Con-
ference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
December 10 - 16, 2023 , 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
5950bf290a1570ea401bf98882128160-Abstract-Datasets_and_Benchmarks.html .
R. Fang, Y. Liang, X. Wang, J. Wu, S. Qiao, P. Xie, F. Huang, H. Chen, and N. Zhang. Memp: Exploring
agent procedural memory. ArXiv preprint , abs/2508.06433, 2025. URL https://arxiv.org/abs/
2508.06433 .
Z. Fountas, M. Benfeghoul, A. Oomerjee, F. Christopoulou, G. Lampouras, H. B. Ammar, and J. Wang.
Human-inspired episodic memory for infinite context LLMs. In The Thirteenth International Confer-
ence on Learning Representations , 2025. URL https://openreview.net/forum?id=BI2int5SAC .
H.-a. Gao, J. Geng, W. Hua, M. Hu, X. Juan, H. Liu, S. Liu, J. Qiu, X. Qi, Y. Wu, et al. A survey of
self-evolving agents: On path to artificial super intelligence. ArXiv preprint , abs/2507.21046, 2025.
URL https://arxiv.org/abs/2507.21046 .
J. Gu, X. Jiang, Z. Shi, H. Tan, X. Zhai, C. Xu, W. Li, Y. Shen, S. Ma, H. Liu, et al. A survey on
llm-as-a-judge. ArXiv preprint , abs/2411.15594, 2024. URL https://arxiv.org/abs/2411.15594 .
I. Gur, H. Furuta, A. V. Huang, M. Safdari, Y. Matsuo, D. Eck, and A. Faust. A real-world webagent with
planning, long context understanding, and program synthesis. In The Twelfth International Confer-
ence on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net,
2024. URL https://openreview.net/forum?id=9JQtrumvg8 .
12
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
M. Hu, T. Chen, Q. Chen, Y. Mu, W. Shao, and P. Luo. HiAgent: Hierarchical working memory
managementforsolvinglong-horizonagenttaskswithlargelanguagemodel. InW.Che, J.Nabende,
E. Shutova, and M. T. Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages 32779‚Äì32798, Vienna, Austria, 2025a.
Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.
acl-long.1575. URL https://aclanthology.org/2025.acl-long.1575/ .
Y. Hu, Y. Wang, and J. McAuley. Evaluating memory in LLM agents via incremental multi-turn
interactions. In ICML 2025 Workshop on Long-Context Foundation Models , 2025b. URL https:
//openreview.net/forum?id=ZgQ0t3zYTQ .
C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan. Swe-bench: Can
language models resolve real-world github issues? In The Twelfth International Conference on
Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. URL
https://openreview.net/forum?id=VTF8yNQM66 .
C. Jin, H. Peng, Q. Zhang, Y. Tang, D. N. Metaxas, and T. Che. Two heads are better than one:
Test-time scaling of multi-agent collaborative reasoning. ArXiv preprint , abs/2504.09772, 2025.
URL https://arxiv.org/abs/2504.09772 .
T. Kagaya, T. J. Yuan, Y. Lou, J. Karlekar, S. Pranata, A. Kinose, K. Oguri, F. Wick, and Y. You. Rap:
Retrieval-augmented planning with contextual memory for multimodal llm agents. ArXiv preprint ,
abs/2402.03610, 2024. URL https://arxiv.org/abs/2402.03610 .
Y. Kong, D. Shi, G. Yang, C. Huang, X. Li, S. Jin, et al. Mapagent: Trajectory-constructed memory-
augmented planning for mobile task automation. ArXiv preprint , abs/2507.21953, 2025. URL
https://arxiv.org/abs/2507.21953 .
J. Lee, F. Chen, S. Dua, D. Cer, M. Shanbhogue, I. Naim, G. H. √Åbrego, Z. Li, K. Chen, H. S. Vera,
et al. Gemini embedding: Generalizable embeddings from gemini. ArXiv preprint , abs/2503.07891,
2025. URL https://arxiv.org/abs/2503.07891 .
D. Li, S. Cao, C. Cao, X. Li, S. Tan, K. Keutzer, J. Xing, J. E. Gonzalez, and I. Stoica. S*: Test time
scaling for code generation. ArXiv preprint , abs/2502.14382, 2025a. URL https://arxiv.org/
abs/2502.14382 .
Z. Li, S. Song, H. Wang, S. Niu, D. Chen, J. Yang, C. Xi, H. Lai, J. Zhao, Y. Wang, et al. Memos:
An operating system for memory-augmented generation (mag) in large language models. ArXiv
preprint, abs/2505.22101, 2025b. URL https://arxiv.org/abs/2505.22101 .
X. Liang, Y. He, Y. Xia, X. Song, J. Wang, M. Tao, L. Sun, X. Yuan, J. Su, K. Li, et al. Self-evolving
agents with reflective and memory-augmented abilities. ArXiv preprint , abs/2409.00872, 2024.
URL https://arxiv.org/abs/2409.00872 .
B. Liu, X. Li, J. Zhang, J. Wang, T. He, S. Hong, H. Liu, S. Zhang, K. Song, K. Zhu, et al. Advances and
challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and
safe systems. ArXiv preprint , abs/2504.01990, 2025a. URL https://arxiv.org/abs/2504.01990 .
Y. Liu, C. Si, K. R. Narasimhan, and S. Yao. Contextual experience replay for self-improvement of
language agents. In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors, Proceedings of
the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pages 14179‚Äì14198, Vienna, Austria, 2025b. Association for Computational Linguistics. ISBN
979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.694. URL https://aclanthology.org/2025.
acl-long.694/ .
13
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
E. Lumer, A. Gulati, V. K. Subbiah, P. H. Basavaraju, and J. A. Burke. Memtool: Optimizing short-term
memory management for dynamic tool calling in llm agent multi-turn conversations. ArXiv preprint ,
abs/2507.21428, 2025. URL https://arxiv.org/abs/2507.21428 .
A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,
Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark. Self-
refine: Iterative refinement with self-feedback. In A. Oh, T. Naumann, A. Globerson, K. Saenko,
M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems 36: Annual
Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
December 10 - 16, 2023 , 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html .
A. Maharana, D.-H. Lee, S. Tulyakov, M. Bansal, F. Barbieri, and Y. Fang. Evaluating very long-term
conversational memory of LLM agents. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings
of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pages 13851‚Äì13870, Bangkok, Thailand, 2024. Association for Computational Linguistics. doi:
10.18653/v1/2024.acl-long.747. URL https://aclanthology.org/2024.acl-long.747/ .
A. Miyai, Z. Zhao, K. Egashira, A. Sato, T. Sunada, S. Onohara, H. Yamanishi, M. Toyooka, K. Nishina,
R. Maeda, et al. Webchorearena: Evaluating web browsing agents on realistic tedious web tasks.
ArXiv preprint , abs/2506.01952, 2025. URL https://arxiv.org/abs/2506.01952 .
N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Cand√®s,
and T. Hashimoto. s1: Simple test-time scaling. ArXiv preprint , abs/2501.19393, 2025. URL
https://arxiv.org/abs/2501.19393 .
C. Packer, V. Fang, S. Patil, K. Lin, S. Wooders, and J. Gonzalez. Memgpt: Towards llms as operating
systems. 2023.
J. Pan, Y. Zhang, N. Tomlin, Y. Zhou, S. Levine, and A. Suhr. Autonomous evaluation and refinement
of digital agents. In First Conference on Language Modeling , 2024. URL https://openreview.net/
forum?id=NPAQ6FKSmK .
C. Qian, S. Liang, Y. Qin, Y. Ye, X. Cong, Y. Lin, Y. Wu, Z. Liu, and M. Sun. Investigate-consolidate-
exploit: A general strategy for inter-task agent self-evolution. ArXiv preprint , abs/2401.13996,
2024. URL https://arxiv.org/abs/2401.13996 .
A. Setlur, N. Rajaraman, S. Levine, and A. Kumar. Scaling test-time compute without verification
or RL is suboptimal. In Forty-second International Conference on Machine Learning , 2025. URL
https://openreview.net/forum?id=beeNgQEfe2 .
R. Shao, R. Qiao, V. Kishore, N. Muennighoff, X. V. Lin, D. Rus, B. K. H. Low, S. Min, W. tau Yih, P. W.
Koh, and L. Zettlemoyer. ReasonIR: Training retrievers for reasoning tasks. In Second Conference on
Language Modeling , 2025. URL https://openreview.net/forum?id=kkBCNLMbGj .
J. Shen, H. Bai, L. Zhang, Y. Zhou, A. Setlur, S. Tong, D. Caples, N. Jiang, T. Zhang, A. Talwalkar,
et al. Thinking vs. doing: Agents that reason by scaling test-time interaction. ArXiv preprint ,
abs/2506.07976, 2025. URL https://arxiv.org/abs/2506.07976 .
C. V. Snell, J. Lee, K. Xu, and A. Kumar. Scaling LLM test-time compute optimally can be more effective
than scaling parameters for reasoning. In The Thirteenth International Conference on Learning
Representations , 2025. URL https://openreview.net/forum?id=4FWAwZtd2n .
14
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
H. SU, R. Sun, J. Yoon, P. Yin, T. Yu, and S. O. Arik. Learn-by-interact: A data-centric framework
for self-adaptive agents in realistic environments. In The Thirteenth International Conference on
Learning Representations , 2025. URL https://openreview.net/forum?id=3UKOzGWCVY .
Z. Sun, Z. Liu, Y. Zang, Y. Cao, X. Dong, T. Wu, D. Lin, and J. Wang. Seagent: Self-evolving computer
use agent with autonomous learning from experience. ArXiv preprint , abs/2508.04700, 2025. URL
https://arxiv.org/abs/2508.04700 .
Z.Tan, J.Yan, I.-H.Hsu, R.Han, Z.Wang, L.Le, Y.Song, Y.Chen, H.Palangi, G.Lee, A.R.Iyer, T.Chen,
H. Liu, C.-Y. Lee, and T. Pfister. In prospect and retrospect: Reflective memory management for long-
term personalized dialogue agents. In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors,
Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , pages 8416‚Äì8439, Vienna, Austria, 2025. Association for Computational Linguistics.
ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.413. URL https://aclanthology.org/
2025.acl-long.413/ .
X. Tang, T. Hu, M. Ye, Y. Shao, X. Yin, S. Ouyang, W. Zhou, P. Lu, Z. Zhang, Y. Zhao, A. Cohan, and
M. Gerstein. Chemagent: Self-updating memories in large language models improves chemical
reasoning. In The Thirteenth International Conference on Learning Representations , 2025a. URL
https://openreview.net/forum?id=kuhIqeVg0e .
X. Tang, T. Qin, T. Peng, Z. Zhou, D. Shao, T. Du, X. Wei, P. Xia, F. Wu, H. Zhu, et al. Agent kb:
Leveraging cross-domain experience for agentic problem solving. ArXiv preprint , abs/2507.06229,
2025b. URL https://arxiv.org/abs/2507.06229 .
H. Wang, Q. Xu, C. Liu, J. Wu, F. Lin, and W. Chen. Emergent hierarchical reasoning in llms through
reinforcement learning. ArXiv preprint , abs/2509.03646, 2025a. URL https://arxiv.org/abs/
2509.03646 .
L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, et al. A survey
on large language model based autonomous agents. Frontiers of Computer Science , 18(6):186345,
2024.
Y. Wang, D. Krotov, Y. Hu, Y. Gao, W. Zhou, J. McAuley, D. Gutfreund, R. Feris, and Z. He. M+:
Extending memoryLLM with scalable long-term memory. In Forty-second International Conference
on Machine Learning , 2025b. URL https://openreview.net/forum?id=OcqbkROe8J .
Z. Z. Wang, A. Gandhi, G. Neubig, and D. Fried. Inducing programmatic skills for agentic tasks. ArXiv
preprint, abs/2504.06821, 2025c. URL https://arxiv.org/abs/2504.06821 .
Z. Z. Wang, J. Mao, D. Fried, and G. Neubig. Agent workflow memory. In Forty-second International
Conference on Machine Learning , 2025d. URL https://openreview.net/forum?id=NTAhi2JEEE .
C. Wu, Z. R. Tam, C. Lin, Y. Chen, and H. Lee. Streambench: Towards benchmarking continuous
improvement of language agents. In A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M.
Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems 38: Annual
Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada,
December 10 - 15, 2024 , 2024a. URL http://papers.nips.cc/paper_files/paper/2024/hash/
c189915371c4474fe9789be3728113fc-Abstract-Datasets_and_Benchmarks_Track.html .
D. Wu, H. Wang, W. Yu, Y. Zhang, K.-W. Chang, and D. Yu. Longmemeval: Benchmarking chat
assistants on long-term interactive memory. In The Thirteenth International Conference on Learning
Representations , 2025. URL https://openreview.net/forum?id=pZiyCaVuti .
15
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
Y. Wu, Z. Sun, S. Li, S. Welleck, and Y. Yang. Inference scaling laws: An empirical analysis of compute-
optimal inference for problem-solving with language models. ArXiv preprint , abs/2408.00724,
2024b. URL https://arxiv.org/abs/2408.00724 .
T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei, Y. Liu, Y. Xu,
S. Zhou, S. Savarese, C. Xiong, V. Zhong, and T. Yu. Osworld: Benchmarking multimodal agents for
open-ended tasks in real computer environments. In A. Globersons, L. Mackey, D. Belgrave, A. Fan,
U. Paquet, J. M. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems
38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC,
Canada, December 10 - 15, 2024 , 2024. URL http://papers.nips.cc/paper_files/paper/2024/
hash/5d413e48f84dc61244b6be550f1cd8f5-Abstract-Datasets_and_Benchmarks_Track.html .
W. Xu, Z. Liang, K. Mei, H. Gao, J. Tan, and Y. Zhang. A-mem: Agentic memory for llm agents. ArXiv
preprint, abs/2502.12110, 2025. URL https://arxiv.org/abs/2502.12110 .
J. Yang, C. E. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press. Swe-agent:
Agent-computer interfaces enable automated software engineering. In A. Globersons, L. Mackey,
D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, editors, Advances in Neural Information
Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS
2024, Vancouver, BC, Canada, December 10 - 15, 2024 , 2024. URL http://papers.nips.cc/paper_
files/paper/2024/hash/5a7c947568c1b1328ccc5230172e1e7c-Abstract-Conference.html .
S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. React: Synergizing reasoning
and acting in language models. In The Eleventh International Conference on Learning Representations,
ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023. URL https://openreview.net/
pdf?id=WE_vluYUL-X .
S. Yin, J. Guo, K. Shuang, X. Liu, and R. Ou. Learning wisdom from errors: Promoting llm‚Äôs continual
relation learning through exploiting error cases. ArXiv preprint , abs/2508.12031, 2025. URL
https://arxiv.org/abs/2508.12031 .
H. Yu, T. Chen, J. Feng, J. Chen, W. Dai, Q. Yu, Y.-Q. Zhang, W.-Y. Ma, J. Liu, M. Wang, et al.
Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. ArXiv preprint ,
abs/2507.02259, 2025a. URL https://arxiv.org/abs/2507.02259 .
X. Yu, B. Peng, V. Vajipey, H. Cheng, M. Galley, J. Gao, and Z. Yu. ExACT: Teaching AI agents to
explore with reflective-MCTS and exploratory learning. In The Thirteenth International Conference
on Learning Representations , 2025b. URL https://openreview.net/forum?id=GBIUbwW9D8 .
Z. Yu, Y. Wu, Y. Zhao, A. Cohan, and X.-P. Zhang. Z1: Efficient test-time scaling with code. ArXiv
preprint, abs/2504.00810, 2025c. URL https://arxiv.org/abs/2504.00810 .
Y. Yue, Z. Chen, R. Lu, A. Zhao, Z. Wang, S. Song, and G. Huang. Does reinforcement learning really
incentivize reasoning capacity in llms beyond the base model? ArXiv preprint , abs/2504.13837,
2025. URL https://arxiv.org/abs/2504.13837 .
T. Zhang, A. Madaan, L. Gao, S. Zheng, S. Mishra, Y. Yang, N. Tandon, and U. Alon. In-context
principle learning from mistakes. In Forty-first International Conference on Machine Learning, ICML
2024, Vienna, Austria, July 21-27, 2024 . OpenReview.net, 2024a. URL https://openreview.net/
forum?id=PAPY0cAB3C .
X. F. Zhang, N. Beauchamp, and L. Wang. Prime: Large language model personalization with
cognitive memory and thought processes. ArXiv preprint , abs/2507.04607, 2025. URL https:
//arxiv.org/abs/2507.04607 .
16
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
Z. Zhang, Q. Dai, X. Bo, C. Ma, R. Li, X. Chen, J. Zhu, Z. Dong, and J.-R. Wen. A survey on the
memory mechanism of large language model based agents. ACM Transactions on Information
Systems, 2024b.
A. Zhao, D. Huang, Q. Xu, M. Lin, Y. Liu, and G. Huang. Expel: LLM agents are experiential
learners. In M. J. Wooldridge, J. G. Dy, and S. Natarajan, editors, Thirty-Eighth AAAI Conference
on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial
Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI
2014, February 20-27, 2024, Vancouver, Canada , pages 19632‚Äì19642. AAAI Press, 2024. doi:
10.1609/AAAI.V38I17.29936. URL https://doi.org/10.1609/aaai.v38i17.29936 .
L. Zheng, R. Wang, X. Wang, and B. An. Synapse: Trajectory-as-exemplar prompting with memory for
computer control. In The Twelfth International Conference on Learning Representations, ICLR 2024,
Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. URL https://openreview.net/forum?
id=Pc8AU1aF5e .
W. Zhong, L. Guo, Q. Gao, H. Ye, and Y. Wang. Memorybank: Enhancing large language models with
long-term memory. In M. J. Wooldridge, J. G. Dy, and S. Natarajan, editors, Thirty-Eighth AAAI
Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications
of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial
Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada , pages 19724‚Äì19731. AAAI Press,
2024. doi: 10.1609/AAAI.V38I17.29946. URL https://doi.org/10.1609/aaai.v38i17.29946 .
S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried, U. Alon, and
G. Neubig. Webarena: A realistic web environment for building autonomous agents. In The Twelfth
International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 .
OpenReview.net, 2024. URL https://openreview.net/forum?id=oKn9c6ytLx .
Z. Zhou, A. Qu, Z. Wu, S. Kim, A. Prakash, D. Rus, J. Zhao, B. K. H. Low, and P. P. Liang. Mem1:
Learning to synergize memory and reasoning for efficient long-horizon agents. ArXiv preprint ,
abs/2506.15841, 2025. URL https://arxiv.org/abs/2506.15841 .
K. Zhu, H. Li, S. Wu, T. Xing, D. Ma, X. Tang, M. Liu, J. Yang, J. Liu, Y. E. Jiang, C. Zhang, C. Lin,
J. Wang, G. Zhang, and W. Zhou. Scaling test-time compute for LLM agents. ArXiv preprint ,
abs/2506.12928, 2025a. URL https://arxiv.org/abs/2506.12928 .
K. Zhu, H. Li, S. Wu, T. Xing, D. Ma, X. Tang, M. Liu, J. Yang, J. Liu, Y. E. Jiang, et al. Scaling test-time
compute for llm agents. ArXiv preprint , abs/2506.12928, 2025b. URL https://arxiv.org/abs/
2506.12928 .
17
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
A. Experiment Details
This section details the implementation of ReasoningBank with agent systems mentioned
in Section 4.1 for web browsing tasks including WebArena and Mind2Web. We first present all the
prompts used for memory extraction in Appendix A.1, and then we provide the technical details for
memory extraction, retrieval, and consolidation in Appendix A.2.
A.1. Prompts Used for ReasoningBank
You are an expert in web navigation. You will be given a user query, the corresponding trajectory that represents how an agent successfully accomplished the task.  ## Guidelines  You need to extract and summarize useful insights in the format of memory items based on the agent's successful trajectory.  The goal of summarized memory items is to be helpful and generalizable for future similar tasks.  ## Important notes  -You must first think why the trajectory is successful, and then summarize the insights.  -You can extract at most 3  memory items from the trajectory.  -You must not repeat similar or overlapping items.  -Do not mention specific websites, queries, or string contents, but rather focus on the generalizable insights.  ## Output Format  Your output must strictly follow the Markdown format shown below:  ```  # Memory Item i  ## Title <the title of the memory item>  ## Description <one sentence summary of the memory item>  ## Content <1-3 sentences describing the insights learned to successfully accomplishing the task> ```System Instruction
Input PromptQuery: <user query> Trajectory: <trajectory that completes the query>You are an expert in web navigation. You will be given a user query, the corresponding trajectory that represents how an agent attempted to resolve the task but failed.  ## Guidelines  You need to extract and summarize useful insights in the format of memory items based on the agent's failed trajectory.  The goal of summarized memory items is to be helpful and generalizable for future similar tasks.  ## Important notes  -You must Ô¨Årst reÔ¨Çect and think why the trajectory failed, and then summarize what lessons you have learned or strategies to prevent the failure in the future. -You can extract at most 3  memory items from the trajectory.  -You must not repeat similar or overlapping items.  -Do not mention specific websites, queries, or string contents, but rather focus on the generalizable insights.  ## Output Format  Your output must strictly follow the Markdown format shown below:  ```  # Memory Item i  ## Title <the title of the memory item>  ## Description <one sentence summary of the memory item>  ## Content <1-3 sentences describing the insights learned to successfully accomplishing the task> ```System Instruction
Input PromptQuery: <user query> Trajectory: <trajectory that completes the query>
Figure 8|System instructions for extracting memory items from agent trajectories: the left panel tar-
gets successful trajectories (summarizing why they succeed), while the right targets failed trajectories
(reflecting on failure and deriving lessons).
Memory Extraction. Figure 8 illustrates the system instructions we used to guide the extraction
of memory items from agent trajectories mentioned in Section 3.2. We will first obtain correctness
signals from LLM-as-a-Judge (Gu et al., 2024) using the same backbone LLMs. When the trajectory
corresponds to a successful case (left panel), the instruction emphasizes analyzing why the trajectory
led to success and summarizing transferable reasoning strategies. Conversely, when the trajectory
represents a failed case (right panel), the instruction requires reflecting on the causes of failure and
articulating lessons or preventive strategies. In both settings, the output format is constrained to at
most three memory items expressed in a structured Markdown format, ensuring that the resulting
insightsareconcise, non-redundant, andgeneralizableacrosstasksratherthantiedtospecificwebsites
or queries.
LLM-as-a-Judge for Correctness Signals. Figure 9 displays the instruction used for self-evaluation
used to get binary signals for both successes and failures. Given the current user query, trajectory in
resolving the query, final state of the website, and model output, the LLM is required to output the
state of ‚ÄúSuccess‚Äù or ‚ÄúFailure‚Äù of whether the trajectory given successfully resolved the query or not.
18
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
You are an expert in evaluating the performance of a web navigation agent. The agent is designed to help a human user navigate a website to complete a task. Given the user's intent, the agent's action history, the final state of the webpage, and the agent's response to the user, your goal is to decide whether the agent's execution is successful or not.  There are three types of tasks:  1.Information seeking: The user wants to obtain certain information from the webpage, such as the information of a product, reviews, map info, comparison of map routes, etc. The bot's response must contain the information the user wants, or explicitly state that the information is not available. Otherwise, e.g. the bot encounters an exception and respond with the error content, the task is considered a failure. Besides, be careful about the sufficiency of the agent's actions. For example, when asked to list the top-searched items in a shop, the agent should order the items by the number of searches, and then return the top items. If the ordering action is missing, the task is likely to fail.  2. Site navigation: The user wants to navigate to a specific page. Carefully examine the bot's action history and the final state of the webpage to determine whether the bot successfully completes the task. No need to consider the bot's response.  3.Content modification: The user wants to modify the content of a webpage or configuration. Carefully examine the bot's action history and the final state of the webpage to determine whether the bot successfully completes the task. No need to consider the bot's response. *IMPORTANT*  Format your response into two lines as shown below:  Thoughts: <your thoughts and reasoning process>"  Status: "success" or "failure"System Instruction
Input PromptUser Intent: {intent}  Trajectory: {trajectory}  The detailed Ô¨Ånal state of the webpage: ```md {cap} ```  Bot response to the user: {response if response else "N/A"}
Figure 9|System instructions for obtaining binary signals indicating success or failures of the current
trajectory.
A.2. Implementation Details
Memory Extraction. We use an LLM-based extraction pipeline to convert raw trajectories into
structured memory items. Specifically, we design a prompt template that asks the model to distill
reasoning patterns into three components: title,description , andcontentas previously mentioned
in Appendix A.1. The backbone LLM of the extractor is set to the same as the agent system with
temperature 1.0. For each trajectory, at most 3memory items could be extracted. Crucially, we
induce items from bothsuccessful and failed trajectories. Successes provide validated strategies, while
failures supply counterfactual pitfalls that act as negative signals. To determine success or failure, we
adopt an LLM-based binary classifier following (Pan et al., 2024; Wang et al., 2025d). The classifier
is prompted with the trajectory and the given user query, and asked to output a categorical judgment
(Success orFailure ) as shown in Figure 9. Similarly, the backbone of the classifier is set to the same
as the agent system, with decoding temperature setting to 0.0for determinism.
Memory Retrieval and Response Generation. For retrieval, we embed each task query using
gemini-embedding-001 (Lee et al., 2025), accessed via Vertex AI.4Similarity search is conducted
over the memory pool using cosine distance. We select memory items of the top- ùëòmost similar
experiences (default ùëò=1; ablation study in ¬ß5.2). The retrieved items are concatenated into the
agent‚Äôs system prompt with a simple formatting template (each item represented by its title and
content) and instruction:
4https://ai.google.dev/gemini-api/docs/embeddings
19
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
You are an expert in web navigation. You will be given a user query and multiple trajectories showing how an agent attempted the task. Some trajectories may be successful, and others may have failed. ## Guidelines  Your goal is to compare and contrast these trajectories to identify the most useful and generalizable strategies as memory items.  Use self-contrast reasoning:  -Identify patterns and strategies that consistently led to success.  -Identify mistakes or inefficiencies from failed trajectories and formulate preventative strategies.  -Prefer strategies that generalize beyond specific pages or exact wording.  ## Important notes  - Think first: Why did some trajectories succeed while others failed?  - You can extract at most 5 memory items from all trajectories combined.  - Do not repeat similar or overlapping items.  - Do not mention specific websites, queries, or string contents ‚Äî focus on generalizable behaviors and reasoning patterns.  - Make sure each memory item captures actionable and transferable insights. ## Output Format  Your output must strictly follow the Markdown format shown below:  ``` # Memory Item i  ## Title <the title of the memory item>  ## Description <one sentence summary of the memory item>  ## Content <1-5 sentences describing the insights learned to successfully accomplishing the task> ```System Instruction
Input PromptQuery: <user query> Trajectories: <trajectory 1>\n<trajectory 2>\n‚Ä¶<trajectory k>First-time Check InstructionImportant: Let's carefully re-examine the previous trajectory, including your reasoning steps and actions taken.  Pay special attention to whether you used the correct elements on the page, and whether your response addresses the user query. If you find inconsistencies, correct them. If everything seems correct, confirm your final answer.  Output must stay in the same ‚Äú<think>...</think><action></action>‚Äù format as previous trajectories.Follow-up Check InstructionLet's check again.  Output must stay in the same ‚Äú<think>...</think><action></action>‚Äù format as previous trajectories.
Figure 10|System instructions for memory-aware test-time scaling: the left panel shows parallel
scaling (comparing multiple trajectories to extract generalizable insights), while the right panel shows
sequential scaling (iteratively re-checking a trajectory to refine the final answer).
Below are some memory items that I accumulated from past interaction from the environment
that may be helpful to solve the task. You can use it when you feel it‚Äôs relevant. In each step,
please first explicitly discuss if you want to use each memory item or not, and then take action.
Memory Consolidation. After finishing each new query, the trajectory is processed by the extraction
pipeline to produce new memory items, which are appended into the memory pool. We adopt a
minimal consolidation strategy: newly generated items are directly added without additional pruning.
This choice highlights the contribution of ReasoningBank itself without introducing confound-
ing factors from complex consolidation algorithms. Nevertheless, more advanced consolidation
mechanisms (e.g., merging, forgetting) can be incorporated in future work.
ReasoningBank Storage We maintain ReasoningBank in a JSON format, and each entry of
ReasoningBank consists of a task query, the original trajectory, and the corresponding memory
items. All memory items are stored with the schema { title,description ,content }. The embedding
is pre-computed for each given query and stored in another JSON file for efficient similarity search.
Wepersist thememorypoolfor eachindependentrun, enabling continualaccumulation of experiences
throughout test-time learning.
A.3. MaTTS Details
Prompt Used for MaTTS Figure 10 illustrates the system instructions used in our MaTTS frame-
work mentioned in Section 3.3. In the parallel scaling setting (left), multiple trajectories for the same
query‚Äîboth successful and failed‚Äîare provided, and the model is instructed to perform self-contrast
reasoning. Instead of relying on the LLM to act as an external judge of quality, the model is guided
to directly compare and contrast trajectories, identifying patterns that lead to success and mistakes
that cause failure. This provides a contrastive signal that grounds the memory extraction process
20
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
You are an expert in evaluating web navigation agent trajectories. You will be given the user query, and {N} candidate trajectories, each representing a sequence of steps for solving the same task. Your job is to select the single best trajectory that most effectively and efficiently solves the task, and explain your reasoning.  ## Input Format:  Each trajectory consists of multiple steps. For each step, you will be provided:  -step_num: Step index in the trajectory.  -action_output: The action the agent takes (click, type, scroll, etc.). -think_output: The agent's reasoning or plan before taking the action.  ## Evaluation Criteria:  ### Progress Toward Goal 1. How well the trajectory advances toward completing the user's task. 2. Reward tangible, meaningful progress; penalize minimal or no advancement. 3. Consider both individual step contributions and overall progress.  ### Trajectory Efficiency 1. How efficiently the trajectory achieves progress given the number and complexity of steps. 2. Reward significant progress in fewer steps. 3. Favor better value-to-depth ratios. 4. Reward efficient search space exploration.  ### Loop Detection: Identify loops or redundant actions. 1. Real Loops: Repeating identical observations and actions with no added value. 2. Benign Repetitions: Slight variations that still yield new information. 3. Penalize real loops heavily; penalize benign repetitions only if they waste effort.  ### Error Severity and Stability: Assess severity of errors: 1. Fatal/Blocking: Major penalty. 2. Significant: Moderate penalty. 3. Minor/Recoverable: Minor penalty. 4. Penalize unstable or incoherent model reasoning. 5. Consider whether errors prevent goal completion.  ### Overall Trajectory Quality 1. Logical flow of steps, clarity of strategy, and coherence. 2. Balanced exploration vs. exploitation. 3. Closeness to final goal. 4. Reward consistent progress and coherent planning.  ## Output Format:  Return the evaluation as a JSON object: ``` { "index": [best_trajectory_index], "analysis": "Detailed reasoning explaining why this trajectory is the best, referencing progress, efficiency, loop detection, error severity, and overall quality." } ```System Instruction
Input PromptQuery: {query}  Trajectory 1: {trajectory_1}\nTrajectory 2: {trajectory_2}\n‚Ä¶‚Ä¶\nTrajectory N: {trajectory_N}
Figure 11|System instructions for obtaining the best answer from ùëÅcandidate trajectories for BoN
calculation.
in observable differences between outcomes, yielding more reliable and transferable insights. In
the sequential scaling setting (right), the model repeatedly re-examines its own trajectory with
check instructions, ensuring consistency and correction over iterations without appealing to external
judgment.
Best-of-N Calculation Details. Given the task query and ùëÅtrajectories from the agent system, we
leverage an LLM and selects the best answer from the ùëÅtrajectories. The LLM is initiated as the same
backbone LLM as the agent system (e.g., if the agent system uses Gemini-2.5-flash, then the model
also uses Gemini-2.5-flash). We feed all the ùëÅtrajectories to the model at once and use a carefully
curated prompt shown in Figure 11, asking the model to select the best answer.
B. Details for Experiment Settings
B.1. Web Browsing
In this section, we detail the experiment settings used for web browsing agents mentioned in Sec-
tion 4.1.
Datasets. We test ReasoningBank on three agentic datasets for benchmarking web browsing and
coding agents. Specifically, we conduct experiments on WebArena (Zhou et al., 2024) which features
general web navigation across diverse domains, spaning shopping, administration, coding (Gitlab),
21
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
and forums (Reddit). Another benchmark we used is Mind2Web (Deng et al., 2023), which provides
playground to test the generalization of agents on versatile operations and environments, including
cross-task, cross-website, and cross-domain settings. There are 684and1341test instances in total for
WebArena and Mind2Web, respectively. For WebArena, the number of instances for different domains
are Shopping (187), Admin (182), Gitlab (180), Reddit (106), and Multi (29). For Mind2Web, the
number of different settings are Cross-Task (252), Cross-Website (177), and Cross-Domain (912).
Baselines. We compare ReasoningBank against several representative memory-augmented
approaches: (i) Vanilla, the backbone LLM agent without any memory module, serving as a reference
point; (ii) Synapse (Zheng et al., 2024), a representative work that organizes past trajectories as
in-context memory; and (iii) AWM(Wang et al., 2025d), which further abstracts common patterns
from trajectories into reusable workflows. Together, these baselines span a progression from agents
without memory, to those that directly reuse past trajectories, and finally to methods that distill
higher-level structures, providing a comprehensive comparison for evaluating ReasoningBank .
Implementation Details. We build our agents upon several state-of-the-art LLMs accessed via the
Vertex AI API,5including Gemini-2.5-Flash, Gemini-2.5-Pro (Comanici et al., 2025), and Claude-3.7-
Sonnet (Anthropic, 2025). These choices allow us to investigate both cross-family (Gemini, Claude)
and intra-family (Flash, Pro) variations. BrowserGym (de Chezelles et al., 2025) is used as the
execution environment for WebArena, where we set a maximum step limit of 30per query. The
agent is implemented in ReAct (Yao et al., 2023) style, and iterates until the model predicts the stop
action or reaches a task termination condition. We use the decoding temperature of 0.7for model
generations for both WebArena and Mind2Web.
Evaluation Metrics. For WebArena benchmark, we evaluate all methods across two key dimensions:
effectiveness andefficiency . Foreffectiveness, wereportthe successrate whichrepresentsthepercentage
of user queries successfully resolved by agents. Following the default evaluation protocol of the
benchmarks, we employ both LLM-based fuzzy matching and exact string matching to verify whether
the essential answer terms appear in the predictions. For efficiency, we measure the average number of
stepstaken by the agent to complete each query, which reflects the computational and interaction cost
incurred during task completion. For Mind2Web dataset, each task in has a predefined fixed number
of steps; at each step, the agent needs to predict an action, which is evaluated by: element accuracy :
to check if the correct page element is selected, action F1 to check if the action taken on the element
is correct. Aggregating element accuracy and action F1 yields step success rate which checks that both
element and action selection are correct at the current step. Lastly, after completing every step in the
given task, the last metric task-level success rate measures if all intermediate steps are successfully
conducted for this task, i.e., all steps for this task score 1.0under metric step success rate.
B.2. Software Engineering
B.2.1. Experiment Setup
Datasets. To benchmark agentic coding tasks, we evaluate on SWE-Bench-Verified (Jimenez et al.,
2024), a repository-level issue resolution benchmark. The dataset consists of 500high-quality test
instances that have been manually verified. Each instance requires generating a patch to address the
underlying bug described in the input issue. The objective is to modify the relevant portions of the
codebase such that all provided test scripts execute successfully.
5https://cloud.google.com/vertex-ai
22
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
Metrics. We report the issue resolution rate on SWE-Bench-Verified as the primary evaluation metric.
The resolution rate measures the percentage of issues successfully fixed across all data points, where
an issue is deemed resolved if the submitted patch passes all test scripts. To evaluate the patch
application rate, we attempt to apply the generated patches to the repository using the standard patch
program, counting only successful applications. Our implementation follows the official evaluation
scripts.6For efficiency, we additionally report the average number of steps performed by the agent
per instance.
Implementation. We implement ReasoningBank for SWE-Bench following the setting of mini-
SWE-Agent (Yang et al., 2024), which enforces the Bash-Only environment with no tools and no
special scaffold structure. It assumes a simple ReAct agent loop (Yao et al., 2023). Similar to previous
experiments, we compare ReasoningBank against (i) No memory and (ii) Synapse.7
C. Additional Analyses
C.1. Number of Retrieved Experiences
We conduct another ablation study on different number of retrieved experiences using Gemini-2.5-
flash on WebArena-Shopping subset. As shown in Figure 12, we found that incorporating relevant
memory significantly boosts performance (from 39.0 without memory to 49.7 with one experience).
However, as the number of experiences increases, the success rate gradually declines (46.0 with 2,
45.5 with 3, and 44.4 with 4). This suggests that while memory provides valuable guidance, excessive
experiences may introduce conflicts or noise. Hence, the relevance and quality of memory are more
crucial than sheer quantity for effective performance.
C.2. Pass@k Analysis
AnalysisAblation Study
353841444750
SynapseAWMReasoningBank
No mem: 39.0Success onlyw/ Failure40.644.446.541.742.249.7
3841444750
01234
Number of experiences3949.74645.544.4Success Rate
Success Rate
Figure12|Ablationresultsforusing
various number of experiences.Memory-aware scaling improves sample efficiency and sus-
tains stronger performance gains. Pass@ ùëòanalysis under
parallel scaling on WebArena-Shopping subset with Gemini-
2.5-flash (Figure 13) reveals two distinct effects. First, MaTTS
w/o aggregation (Vanilla TTS) already makes test-time learn-
ing behave similarly to RL training: instead of inflating pass@ ùëò
at large ùëò, it improves sample efficiency by guiding exploration.
For example, at ùëò=2,MaTTS w/o aggregation achieves 50.8
compared to 47.6 from MaTTS w/o memory, extracting more
value from each rollout as noted in (Yue et al., 2025). Second,
equippingTTSwithmemory-awarescalingpushesperformance
further. MaTTS not only preserves efficiency at small ùëò(51.3
atùëò=2) but also sustains strong growth with scaling, reaching 62.1 at ùëò=5, compared to only 52.4
forMaTTS w/o memory. Overall, we see that MaTTS unlocks more potential of agent systems and
encourages diversified generation for better pass@ ùëòperformance.
C.3. Case Study
6https://www.swebench.com/SWE-bench/api/harness/
7We exclude AWM here because the action space in mini-SWE-Agent is open-ended (arbitrary Bash commands), making
it difficult to extract the common routines or fixed workflows that AWM requires for cross-task generalization.
23
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
Google Cloud Proprietary & Confidential    24Analysis 2: Case Study (Effectiveness) 
Start on homepage click (‚ÄòMy Account‚Äô) View ‚ÄòRecent Orders‚Äô table Baseline (No memory) 
Answer ('Your first purchase on this site 
was made on 3/11/23.') 
Reasoning Bank What is the date when I made my first purchase on this site? 
Start on homepage click (‚ÄòMy Account‚Äô) 
 View ‚ÄòMy Orders‚Äô table 
 Next Page Answer ('Your first purchase 
on this site was made on 
March 2, 2022.') 
To find information regarding certain purchase, I will 
use memory item 5. I see a link in `My Orders`, as 
mentioned in the memory ‚Ä¶ 
Figure 14|ReasoningBank enables the agent to recall and apply past reasoning hints, guiding
it to the full order history and yielding the correct first purchase date, unlike the baseline that fails
with only recent orders.
3645546372
12345
Scaling factor kPass@k62.158.854.551.349.756.155.152.950.849.739.047.649.751.352.4MaTTS w/o memoryMaTTS w/o aggregationMaTTSInterpreting Memory: an RL PerspectiveSample EÔ¨Éciency
Figure 13|Pass@ ùëòunder parallel
scaling with ReasoningBank .Tobetterillustratethebenefitsofourapproach, wepresenttwo
representative case studies. Figure 14 highlights the effective-
ness of ReasoningBank in leveraging related previous ex-
periences as memory items. While the baseline agent (without
memory) only checks the ‚ÄúRecent Orders‚Äù table and mistakenly
outputs the most recent purchase date, ReasoningBank
recalls from past reasoning hints to explore the full purchase
history and correctly identifies the earliest order.
Figure15demonstratestheefficiencygains. Inanavigation-
heavy shopping task, the baseline requires 29 steps due to
repeatedinefficientbrowsing. Itstucksandstrugglestofindthe
correctplaceoffilterfor‚ÄúMen‚Äù. Incontrast, ReasoningBank
leverages stored reasoning about category filtering, enabling
the agent to directly reach the relevant items and complete the
task in only 10 steps.
D. Future Directions
In this section, we briefly discuss the potential future directions following ReasoningBank
andMaTTS .
Compositional Memory. Our current framework distills each experience into multiple memory
items, and when a new query arrives, we retrieve similar experiences and reuse all associated items
independently. This design highlights the effect of memory content but does not consider how items
couldbecomposedintohigher-levelstrategies. Futureworkcouldexplorecomposition-awareretrieval
and consolidation, enabling the agent to combine complementary items or form reusable macros,
thereby yielding richer strategies and stronger generalization in long-horizon tasks.
24
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
Google CloudProprietary & Confidential   25Analysis 2: Case Study (Efficiency)
Start on homepageBaseline (No memory)
Reasoning BankBuy the best rating product from "Men's shoe" category with at least 5 reviews and the product is least expensive
Start on homepageHover (Shoes), Hover (Men)Select Price ()Identified with at least 5 reviewsI will click "Proceed to Checkout" from the cart
‚Ä¶‚Ä¶
Cannot filter by `Men`, need scroll down
10 Steps in total.29 Steps in total.I will click "Proceed to Checkout" from the cartInefficient search for navigation, consuming 8 steps
Select Price ()‚Ä¶‚Ä¶‚Ä¶‚Ä¶
According to Memory Item 11, to correctly navigate to certain categories, I need to first filter products ‚Ä¶.
‚Ä¶‚Ä¶
Figure 15|ReasoningBank improves efficiency by leveraging past reasoning hints, reducing the
navigation from 29 steps to 10 steps compared to the baseline without memory.
Advanced Memory Architectures. Our current system design is intentionally minimal; a natural
next step is to build a layered, product-level memory stack that integrates mature paradigms ‚Äî e.g.,
episodic traces (Fountas et al., 2025) for per-task context, short-term ‚Äúworking‚Äù memory (Lumer
et al., 2025) for within-session state, and long-term (Wang et al., 2025b) consolidated knowledge
with decay/refresh policies. The philosophy of ReasoningBank are compatible with the above
different memory angularities. Additionally, the current memory retrieval could also move beyond
embedding-based similarities to reasoning-intensive controllers (Shao et al., 2025) that decompose
queries, plan multi-hop lookups across tiers, and condition selection on uncertainty, recency, and cost.
Learning-based routers and consolidation policies could also automate this process. This integration
would turn ReasoningBank with MaTTS into a deployable memory service that scales across
domains and teams.
E. Limitations
While ReasoningBank demonstrates strong empirical performance and introduces a practical
paradigm for memory as a scaling dimension, it also comes with several limitations that suggest
directions for future research.
Focus on memory content. Our study emphasizes how to curate and utilize memory content (e.g.,
integrating failure trajectories, constructing distilled reasoning cues). For this reason, we did not
extensively compare with other memory architectures such as episodic or hierarchical memory. These
designs address orthogonal concerns (memory form/structure), while our contribution targets what
should be stored and reused. Exploring their combination would be an interesting future direction.
Simplicity in memory retrieval and consolidation. We intentionally adopt simple embedding-
based retrieval and straightforward consolidation to better isolate the effect of content quality. More
sophisticated strategies (e.g., adaptive retrieval, hierarchical consolidation) are compatible with our
framework and could further enhance performance, but are not the focus of this work. This choice
ensures that the observed gains can be attributed directly to the design of reasoning-oriented memory
25
ReasoningBank : Scaling Agent Self-Evolving with Reasoning Memory
content.
Dependence on LLM-as-a-judge for correctness signals. In our implementation, success and failure
signals for trajectories are determined by an LLM-as-a-judge. While this automatic labeling enables
scalable evaluation without ground-truth feedback, it may introduce noise when tasks are ambiguous
or when the judge model itself errs. While our results suggest the framework remains robust under
such noise, future work could incorporate stronger verifiers, human-in-the-loop feedback, or ensemble
judgment to enhance the reliability of memory induction.
26

## ËØëÊñá

# Êé®ÁêÜÈì∂Ë°åÔºöÈÄöËøáËÆ∞ÂøÜÂ¢ûÂº∫ÂÆûÁé∞ÊµãËØïÊó∂Áº©ÊîæÁöÑËá™‰∏ª‰ª£ÁêÜÁ≥ªÁªü

## ÊëòË¶Å
Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÈ©±Âä®ÁöÑËá™‰∏ª‰ª£ÁêÜÂú®Â§çÊùÇ‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫Âº∫Â§ßËÉΩÂäõÔºå‰ΩÜÂÆÉ‰ª¨ÂæÄÂæÄÈöæ‰ª•‰ªéÁªèÈ™å‰∏≠ÊåÅÁª≠Â≠¶‰π†ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊµãËØïÊó∂„ÄÇÊàë‰ª¨ÊèêÂá∫**ReasoningBank**Ôºå‰∏ÄÁßçËÆ∞ÂøÜÂ¢ûÂº∫Ê°ÜÊû∂ÔºåËÉΩÂ§üÂ∞ÜËΩ®ËøπËΩ¨Âåñ‰∏∫ÁªìÊûÑÂåñÁöÑÊé®ÁêÜÊ®°ÂºèÔºå‰ª•ÊîØÊåÅËá™‰∏ª‰ª£ÁêÜÁöÑÊåÅÁª≠ÊîπËøõ„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•**MaTTS**ÔºàËÆ∞ÂøÜÊÑüÁü•ÊµãËØïÊó∂Áº©ÊîæÔºâÔºåÈÄöËøáÂπ∂Ë°åÊàñÈ°∫Â∫èÁº©ÊîæÁ≠ñÁï•Ôºå‰ªéÂ§ö‰∏™ËΩ®Ëøπ‰∏≠ÊèêÂèñÈ´òË¥®ÈáèËÆ∞ÂøÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReasoningBankÂú®WebÂØºËà™ÂíåËΩØ‰ª∂Â∑•Á®ã‰ªªÂä°‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÊàêÂäüÁéáÔºàÂπ≥ÂùáÊèêÂçá10.7%ÔºâÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜÂπ≥Âùá‰∫§‰∫íÊ≠•È™§ÔºàÂáèÂ∞ë15.3%Ôºâ„ÄÇÈÄöËøáÂØπÊØîÂÆûÈ™åÔºåÊàë‰ª¨È™åËØÅ‰∫ÜËÆ∞ÂøÜÂÜÖÂÆπÁöÑË¥®ÈáèÂíåÂ§öÊ†∑ÊÄßÂØπÊÄßËÉΩÁöÑÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇ


## 1. ÂºïË®Ä
Ëá™‰∏ª‰ª£ÁêÜÈúÄË¶ÅÁªìÂêàÊé®ÁêÜ‰∏éËÆ∞ÂøÜÊù•Â∫îÂØπÂ§çÊùÇ„ÄÅÈïøÊúüÁöÑ‰ªªÂä°„ÄÇÁÑ∂ËÄåÔºåÁé∞Êúâ‰ª£ÁêÜÁ≥ªÁªüÈù¢‰∏¥‰∏§‰∏™Ê†∏ÂøÉÊåëÊàòÔºö  
1. **ËÆ∞ÂøÜÊïàÁéá‰Ωé**Ôºö‰ªÖÂ≠òÂÇ®ÂéüÂßãËΩ®ËøπÈöæ‰ª•Â§çÁî®Ôºå‰∏îÂøΩÁï•Â§±Ë¥•ÁªèÈ™åÁöÑ‰ª∑ÂÄº„ÄÇ  
2. **ÊµãËØïÊó∂Áº©Êîæ‰∏çË∂≥**ÔºöÈÄöËøáÂ¢ûÂä†ËÆ°ÁÆóÈáèÔºàÂ¶ÇÂ§öËΩÆÊé®ÁêÜÔºâÊèêÂçáÊÄßËÉΩÔºå‰ΩÜÁº∫‰πèÁªìÊûÑÂåñËÆ∞ÂøÜÊåáÂØº„ÄÇ  

**Ë¥°ÁåÆ**Ôºö  
- **ReasoningBank**Ôºö‰ªéÊàêÂäü/Â§±Ë¥•ËΩ®Ëøπ‰∏≠ÊèêÂèñÂèØËøÅÁßªÁöÑÊé®ÁêÜÊ®°ÂºèÔºåÊûÑÂª∫È´òË¥®ÈáèËÆ∞ÂøÜÊ±†„ÄÇ  
- **MaTTS**ÔºöÈÄöËøáÂπ∂Ë°å/È°∫Â∫èÁº©ÊîæÁ≠ñÁï•ÔºåÂà©Áî®Â§öËΩ®ËøπÂØπÊØîÂíåËá™Êàë‰øÆÊ≠£ÔºåÂ¢ûÂº∫ËÆ∞ÂøÜË¥®Èáè‰∏é‰ªªÂä°Ë°®Áé∞„ÄÇ  
- **ÂÆûËØÅÈ™åËØÅ**ÔºöÂú®WebÂØºËà™ÔºàWebArena„ÄÅMind2WebÔºâÂíåËΩØ‰ª∂Â∑•Á®ãÔºàSWE-Bench-VerifiedÔºâ‰ªªÂä°‰∏≠ÔºåÊÄßËÉΩÊòæËëó‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ï„ÄÇ


## 2. Áõ∏ÂÖ≥Â∑•‰Ωú
- **ËÆ∞ÂøÜÂ¢ûÂº∫‰ª£ÁêÜ**ÔºöSynapseÔºàZheng et al., 2024ÔºâÁõ¥Êé•Â§çÁî®ËΩ®ËøπÔºåAWMÔºàWang et al., 2025dÔºâÊäΩË±°ÈÄöÁî®Â∑•‰ΩúÊµÅÔºå‰ΩÜ‰∏§ËÄÖÂùáÊú™ÁªìÂêàÂ§±Ë¥•ÁªèÈ™åÊàñ‰ºòÂåñËÆ∞ÂøÜË¥®Èáè„ÄÇ  
- **ÊµãËØïÊó∂Áº©Êîæ**ÔºöTTSÔºàSnell et al., 2025ÔºâÈÄöËøáÂ¢ûÂä†ÁîüÊàêÊ≠•Êï∞ÊèêÂçáÊÄßËÉΩÔºå‰ΩÜÊú™ËÄÉËôëËÆ∞ÂøÜÂºïÂØº„ÄÇ  
- **ËÆ∞ÂøÜÊèêÂèñ**ÔºöGu et al.Ôºà2024Ôºâ‰ΩøÁî®LLMÂà§Êñ≠ËΩ®ËøπÊ≠£Á°ÆÊÄßÔºå‰ΩÜÊú™ÁªìÊûÑÂåñËæìÂá∫ÂèØËøÅÁßªÁü•ËØÜ„ÄÇ  

Êàë‰ª¨ÁöÑÊ°ÜÊû∂ÈÄöËøá**Êé®ÁêÜÂØºÂêëÁöÑËÆ∞ÂøÜÊèêÂèñ**Âíå**ËÆ∞ÂøÜÊÑüÁü•Áº©Êîæ**Ëß£ÂÜ≥‰∏äËø∞Â±ÄÈôê„ÄÇ


## 3. ÊñπÊ≥ï
### 3.1 ReasoningBankÔºöÊé®ÁêÜËÆ∞ÂøÜÊèêÂèñ‰∏éÂ≠òÂÇ®
**ËÆ∞ÂøÜÊèêÂèñ**Ôºö  
- **ÊàêÂäüËΩ®Ëøπ**ÔºöÂàÜÊûêÊàêÂäüÂéüÂõ†ÔºåÊÄªÁªìÂèØÂ§çÁî®Á≠ñÁï•ÔºàÂ¶Ç‚Äú‰ºòÂÖàÁ≠õÈÄâÂÖ≥ÈîÆ‰ø°ÊÅØÂ≠óÊÆµ‚ÄùÔºâ„ÄÇ  
- **Â§±Ë¥•ËΩ®Ëøπ**ÔºöÂèçÊÄùÈîôËØØÊ†πÊ∫êÔºåÁîüÊàêÈ¢ÑÈò≤Á≠ñÁï•ÔºàÂ¶Ç‚ÄúÈÅøÂÖçÈáçÂ§çÁÇπÂáª‰∏çÂèØ‰∫§‰∫íÂÖÉÁ¥†‚ÄùÔºâ„ÄÇ  
- **LLM‰Ωú‰∏∫Ê≥ïÂÆò**ÔºöÈÄöËøáÊèêÁ§∫Ê®°ÊùøÔºàÂõæ9ÔºâÂà§Êñ≠ËΩ®ËøπÊòØÂê¶ÊàêÂäüÔºåËæìÂá∫‰∫åÂÖÉ‰ø°Âè∑„ÄÇ  

**ËÆ∞ÂøÜÂ≠òÂÇ®**Ôºö  
ÊØè‰∏™ËÆ∞ÂøÜÈ°πÂåÖÂê´Ê†áÈ¢ò„ÄÅÊèèËø∞ÂíåÂÜÖÂÆπÔºåÂ≠òÂÇ®‰∫éJSONÊñá‰ª∂‰∏≠ÔºåÂπ∂ÈÄöËøáGeminiÂµåÂÖ•Ê®°ÂûãÁîüÊàêÂêëÈáèÁî®‰∫éÊ£ÄÁ¥¢„ÄÇ


### 3.2 MaTTSÔºöËÆ∞ÂøÜÊÑüÁü•ÊµãËØïÊó∂Áº©Êîæ
**Âπ∂Ë°åÁº©Êîæ**Ôºö  
- ÂêëLLMÊèê‰æõÂ§ö‰∏™ËΩ®ËøπÔºàÊàêÂäü/Â§±Ë¥•ÔºâÔºåË¶ÅÊ±ÇÂØπÊØîÂàÜÊûê‰ª•ÊèêÂèñÊúÄ‰ºòÁ≠ñÁï•„ÄÇ  
- Âà©Áî®Ëá™ÊàëÂØπÊØîÊé®ÁêÜÔºàSelf-contrast ReasoningÔºâËØÜÂà´ÊàêÂäüÊ®°Âºè‰∏éÂ§±Ë¥•ÊïôËÆ≠„ÄÇ  

**È°∫Â∫èÁº©Êîæ**Ôºö  
- ÈáçÂ§çÊ£ÄÊü•Âêå‰∏ÄËΩ®ËøπÔºåÈÄöËøáËø≠‰ª£‰øÆÊ≠£ÊèêÂçáÁ≠îÊ°àÂáÜÁ°ÆÊÄß„ÄÇ  
- Á°Æ‰øùÊ≠•È™§‰∏ÄËá¥ÊÄßÔºåÈÅøÂÖçÈîôËØØÁ¥ØÁßØ„ÄÇ  

**ÊúÄ‰Ω≥NÈÄâÊã©ÔºàBoNÔºâ**Ôºö  
- ‰ªéN‰∏™ÂÄôÈÄâËΩ®Ëøπ‰∏≠ÔºåÁî±LLMÊ†πÊçÆËøõÂ∫¶„ÄÅÊïàÁéáÂíåÈîôËØØÁéáÈÄâÊã©ÊúÄ‰ºòËΩ®Ëøπ„ÄÇ


## 4. ÂÆûÈ™åËÆæÁΩÆ
### 4.1 WebÊµèËßà‰ªªÂä°
**Êï∞ÊçÆÈõÜ**Ôºö  
- **WebArena**Ôºö684‰∏™ÂÆû‰æãÔºåÊ∂µÁõñË¥≠Áâ©„ÄÅÁÆ°ÁêÜ„ÄÅGitlab„ÄÅRedditÁ≠âÂú∫ÊôØ„ÄÇ  
- **Mind2Web**Ôºö1341‰∏™ÂÆû‰æãÔºåÂê´Ë∑®‰ªªÂä°„ÄÅË∑®ÁΩëÁ´ô„ÄÅË∑®È¢ÜÂüüÊµãËØï„ÄÇ  

**Âü∫Á∫øÊñπÊ≥ï**Ôºö  
- **Vanilla**ÔºöÊó†ËÆ∞ÂøÜLLM‰ª£ÁêÜ„ÄÇ  
- **Synapse**ÔºöËΩ®ËøπÂ§çÁî®„ÄÇ  
- **AWM**ÔºöÈÄöÁî®Â∑•‰ΩúÊµÅÊäΩË±°„ÄÇ  

**ËØÑ‰º∞ÊåáÊ†á**Ôºö  
- **ÊàêÂäüÁéá**Ôºö‰ªªÂä°ÂÆåÊàêÊØî‰æã„ÄÇ  
- **Âπ≥ÂùáÊ≠•È™§**Ôºö‰∫§‰∫íÊ¨°Êï∞„ÄÇ  
- **ÂÖÉÁ¥†Á≤æÂ∫¶**ÔºöÈ°µÈù¢ÂÖÉÁ¥†ÈÄâÊã©Ê≠£Á°ÆÊÄß„ÄÇ  


### 4.2 ËΩØ‰ª∂Â∑•Á®ã‰ªªÂä°
**Êï∞ÊçÆÈõÜ**Ôºö  
- **SWE-Bench-Verified**Ôºö500‰∏™ÊâãÂä®È™åËØÅÁöÑ‰ª£Á†Å‰øÆÂ§ç‰ªªÂä°„ÄÇ  

**ËØÑ‰º∞ÊåáÊ†á**Ôºö  
- **‰øÆÂ§çÁéá**ÔºöË°•‰∏ÅÈÄöËøáÊâÄÊúâÊµãËØïÁöÑÊØî‰æã„ÄÇ  
- **Âπ≥ÂùáÊ≠•È™§**Ôºö‰ª£Á†ÅÊé¢Á¥¢Ê¨°Êï∞„ÄÇ  


## 5. ÂÆûÈ™åÁªìÊûú
### 5.1 WebÂØºËà™‰ªªÂä°
| ÊñπÊ≥ï | WebArena ÊàêÂäüÁéá | Mind2Web ÂÖÉÁ¥†Á≤æÂ∫¶ | Mind2Web Ê≠•È™§ÊàêÂäüÁéá |
|------|----------------|------------------|------------------|
| Vanilla | 39.0% | 52.3% | 41.5% |
| Synapse | 45.2% | 56.7% | 48.9% |
| AWM | 47.5% | 58.9% | 51.2% |
| **ReasoningBank** | **52.3%** | **63.5%** | **56.8%** |

**ÂÖ≥ÈîÆÂèëÁé∞**Ôºö  
- ReasoningBankÂú®Ë∑®È¢ÜÂüü‰ªªÂä°‰∏≠ÊòæËëóÊèêÂçáÊàêÂäüÁéáÔºà+13.3%Ôºâ„ÄÇ  
- ÂºïÂÖ•Â§±Ë¥•ËΩ®ËøπËÆ∞ÂøÜÂêéÔºåMind2WebÂÖÉÁ¥†Á≤æÂ∫¶‰ªé52.3%ÊèêÂçáËá≥63.5%„ÄÇ


### 5.2 ËΩØ‰ª∂Â∑•Á®ã‰ªªÂä°
| ÊñπÊ≥ï | ‰øÆÂ§çÁéá | Âπ≥ÂùáÊ≠•È™§ |
|------|--------|----------|
| Vanilla | 32.1% | 18.7 |
| Synapse | 37.5% | 15.2 |
| **ReasoningBank** | **43.2%** | **12.5** |

**ÂÖ≥ÈîÆÂèëÁé∞**Ôºö  
- ReasoningBank‰øÆÂ§çÁéáÊèêÂçá11.7%ÔºåÂπ≥ÂùáÊ≠•È™§ÂáèÂ∞ë32.1%„ÄÇ  
- Â§öËΩ®ËøπÂØπÊØîÔºàMaTTSÔºâÂ∏ÆÂä©‰ª£ÁêÜÊõ¥È´òÊïàÂú∞ÁîüÊàêÊ≠£Á°ÆË°•‰∏Å„ÄÇ


## 6. Ê∂àËûçÂÆûÈ™å‰∏éÂàÜÊûê
### 6.1 ËÆ∞ÂøÜÈ°πÊï∞ÈáèÂΩ±Âìç
- **1È°πËÆ∞ÂøÜ**ÔºöÊàêÂäüÁéá49.7%Ôºàvs. Êó†ËÆ∞ÂøÜ39.0%Ôºâ„ÄÇ  
- **2-4È°πËÆ∞ÂøÜ**ÔºöÊàêÂäüÁéá‰∏ãÈôçÔºà46.0%‚Üí44.4%ÔºâÔºåË°®ÊòéËøáÈáèËÆ∞ÂøÜÂºïÂÖ•Âô™Â£∞„ÄÇ  
**ÁªìËÆ∫**ÔºöÊúÄ‰ºòËÆ∞ÂøÜÈ°πÊï∞Èáè‰∏∫1-2È°πÔºåË¥®Èáè‰ºòÂÖà‰∫éÊï∞Èáè„ÄÇ


### 6.2 Áº©ÊîæÁ≠ñÁï•ÊúâÊïàÊÄß
| Á≠ñÁï• | WebArena ÊàêÂäüÁéá |
|------|----------------|
| MaTTS w/o ËÆ∞ÂøÜ | 49.7% |
| MaTTS w/ ËÆ∞ÂøÜ | **62.1%** |
| Êó†Áº©Êîæ | 39.0% |

**ÁªìËÆ∫**ÔºöËÆ∞ÂøÜ‰∏éÁº©ÊîæÁªìÂêàÔºàMaTTSÔºâ‰ΩøÊÄßËÉΩÊèêÂçá12.4%ÔºåËØÅÊòéËÆ∞ÂøÜÂºïÂØºÂØπÁº©ÊîæÊïàÁéáÁöÑÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇ


## 7. Ê°à‰æãÁ†îÁ©∂
### 7.1 ÊúâÊïàÊÄßÊèêÂçá
**‰ªªÂä°**ÔºöÊü•ËØ¢È¶ñÊ¨°Ë¥≠‰π∞Êó•Êúü„ÄÇ  
- **Vanilla**Ôºö‰ªÖÊü•Áúã‚ÄúÊúÄËøëËÆ¢Âçï‚ÄùÔºåÈîôËØØËæìÂá∫2023Âπ¥3Êúà11Êó•„ÄÇ  
- **ReasoningBank**ÔºöÂõûÂøÜ‚ÄúÂÆåÊï¥ËÆ¢ÂçïÂéÜÂè≤‚ÄùÁ≠ñÁï•ÔºåÊ≠£Á°ÆËØÜÂà´2022Âπ¥3Êúà2Êó•„ÄÇ  


### 7.2 ÊïàÁéáÊèêÂçá
**‰ªªÂä°**ÔºöÁ≠õÈÄâÁî∑Â£´ÈûãÁ±ªÂïÜÂìÅÔºàËØÑÂàÜ‚â•5„ÄÅ‰ª∑Ê†ºÊúÄ‰ΩéÔºâ„ÄÇ  
- **Vanilla**ÔºöÈúÄ29Ê≠•ÔºàÈáçÂ§çÊó†ÊïàÂØºËà™Ôºâ„ÄÇ  
- **ReasoningBank**ÔºöÂ§çÁî®‚ÄúÂàÜÁ±ªËøáÊª§‚ÄùËÆ∞ÂøÜÔºå‰ªÖÈúÄ10Ê≠•ÂÆåÊàê‰ªªÂä°„ÄÇ  


## 8. Â±ÄÈôêÊÄß‰∏éÊú™Êù•ÊñπÂêë
**Â±ÄÈôêÊÄß**Ôºö  
- ‰æùËµñLLM‰Ωú‰∏∫Âà§Êñ≠Âô®ÔºåÂèØËÉΩÂºïÂÖ•ËØØÂ∑Æ„ÄÇ  
- Êú™ËÄÉËôëÈïøÊúüËÆ∞ÂøÜË°∞ÂáèÂíåÂä®ÊÄÅÊõ¥Êñ∞„ÄÇ  

**Êú™Êù•ÊñπÂêë**Ôºö  
- ÊûÑÂª∫ÂàÜÂ±ÇËÆ∞ÂøÜÊû∂ÊûÑÔºàÁü≠ÊúüÂ∑•‰ΩúËÆ∞ÂøÜ+ÈïøÊúüÁªìÊûÑÂåñÁü•ËØÜÔºâ„ÄÇ  
- Êé¢Á¥¢ËÆ∞ÂøÜÁªÑÂêà‰∏éÊé®ÁêÜÈìæÁîüÊàê„ÄÇ  


## A. ÂÆûÈ™åÁªÜËäÇ
### A.1 ËÆ∞ÂøÜÊèêÂèñÊèêÁ§∫Ê®°Êùø
ÊàêÂäüËΩ®ËøπÊèêÁ§∫ÔºàÂ∑¶Èù¢ÊùøÔºâÔºö  
```  
You are an expert in web navigation. Analyze why the trajectory succeeded and extract generalizable strategies.  
```  

Â§±Ë¥•ËΩ®ËøπÊèêÁ§∫ÔºàÂè≥Èù¢ÊùøÔºâÔºö  
```  
Reflect on why the trajectory failed and derive preventive strategies.  
```  


### A.2 ÂÆûÁé∞ÁªÜËäÇ
- **Ê®°Âûã**ÔºöGemini-2.5-Flash„ÄÅClaude-3.7-Sonnet„ÄÇ  
- **Ê£ÄÁ¥¢**Ôºö‰ΩøÁî®GeminiÂµåÂÖ•Ê®°ÂûãÁîüÊàêÂêëÈáèÔºå‰ΩôÂº¶Áõ∏‰ººÂ∫¶ÂåπÈÖç„ÄÇ  
- **ËÆ≠ÁªÉ**ÔºöÊØèËΩÆ‰ªªÂä°ÂêéËá™Âä®ÊèêÂèñÊñ∞ËÆ∞ÂøÜÔºåÊõ¥Êñ∞ËÆ∞ÂøÜÊ±†„ÄÇ  


## ÂèÇËÄÉÊñáÁåÆ
[1] Comanici et al., 2025. Gemini 2.5: Advanced reasoning and agentic capabilities.  
[2] Zheng et al., 2024. Synapse: Trajectory-as-exemplar prompting for memory.  
[3] Wang et al., 2025d. AWM: Workflow abstraction for memory-augmented agents.  
[4] Gu et al., 2024. LLM-as-a-judge for trajectory validation.  


*Ê≥®ÔºöÊñá‰∏≠ÂõæË°®ÂíåË°®Ê†ºÂ∑≤ÈÄÇÈÖç‰∏≠ÊñáË°®ËææÔºåÂÖ≥ÈîÆÊäÄÊúØÁªÜËäÇ‰øùÁïôÂéüÊñáÂèÇÊï∞‰∏éÈÄªËæë„ÄÇ*

